#!/usr/bin/env python
"""
ä½¿ç”¨ Robust Attention Analyzer çš„æ¼”ç¤ºè„šæœ¬
=========================================

è¿™ä¸ªè„šæœ¬ä½¿ç”¨æ–°çš„å¥å£®æ³¨æ„åŠ›åˆ†æç³»ç»Ÿï¼Œèƒ½å¤Ÿï¼š
1. è‡ªåŠ¨è¯Šæ–­æ³¨æ„åŠ›è´¨é‡
2. æ ¹æ®è´¨é‡è‡ªåŠ¨é€‰æ‹©åˆ†æç­–ç•¥
3. å³ä½¿åŸå­æ³¨æ„åŠ›ç›¸åŒä¹Ÿèƒ½æä¾›æœ‰ç”¨åˆ†æ
4. ç”Ÿæˆè¯¦ç»†çš„å¯è§†åŒ–å’Œç»Ÿè®¡æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•:
    python demo_robust_attention.py \
        --model_path /path/to/checkpoint.pt \
        --cif_path /path/to/structure.cif \
        --text "Material description..." \
        --save_dir ./results
"""

import argparse
import torch
from pathlib import Path

from jarvis.core.atoms import Atoms
from jarvis.core.graphs import Graph
from jarvis.core.specie import chem_data, get_node_attributes
import numpy as np

from models.alignn import ALIGNN, ALIGNNConfig
from robust_attention_analyzer import run_complete_analysis


def load_model(checkpoint_path, device='cuda'):
    """åŠ è½½æ¨¡å‹å’Œé…ç½®"""

    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    if 'config' in checkpoint:
        config = checkpoint['config']
        print("âœ… ä»checkpointåŠ è½½é…ç½®")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°configï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = ALIGNNConfig(
            name="alignn",
            alignn_layers=4,
            gcn_layers=4,
            atom_input_features=92,
            hidden_features=256,
            output_features=1,
            use_cross_modal_attention=True,
            cross_modal_hidden_dim=256,
            cross_modal_num_heads=4,
            use_middle_fusion=True,
            use_fine_grained_attention=True,
        )

    print(f"\nğŸ“‹ æ¨¡å‹é…ç½®:")
    print(f"   - use_cross_modal_attention: {config.use_cross_modal_attention}")
    print(f"   - use_middle_fusion: {config.use_middle_fusion}")
    print(f"   - use_fine_grained_attention: {config.use_fine_grained_attention}")

    model = ALIGNN(config)

    checkpoint_state = checkpoint.get('model', checkpoint)
    missing_keys, unexpected_keys = model.load_state_dict(checkpoint_state, strict=False)

    if missing_keys:
        print(f"\nâš ï¸  Missing keys: {len(missing_keys)}")
        if len(missing_keys) <= 5:
            for key in missing_keys:
                print(f"     - {key}")

    if unexpected_keys:
        print(f"\nâš ï¸  Unexpected keys: {len(unexpected_keys)}")
        if len(unexpected_keys) <= 5:
            for key in unexpected_keys:
                print(f"     - {key}")

    model = model.to(device)
    model.eval()

    print(f"\nâœ… æ¨¡å‹å·²åŠ è½½å¹¶è®¾ç½®ä¸º eval æ¨¡å¼")
    print(f"   - Training mode: {model.training}")

    return model, config


def cif_to_graph(cif_path, cutoff=8.0, max_neighbors=12):
    """CIFè½¬å›¾"""

    atoms = Atoms.from_cif(cif_path)

    g, lg = Graph.atom_dgl_multigraph(
        atoms=atoms,
        cutoff=cutoff,
        max_neighbors=max_neighbors,
        atom_features="atomic_number",
        compute_line_graph=True,
        use_canonize=True
    )

    # æ„å»ºç‰¹å¾æŸ¥æ‰¾è¡¨
    max_z = max(v["Z"] for v in chem_data.values())
    template = get_node_attributes("C", atom_features="cgcnn")
    features = np.zeros((1 + max_z, len(template)))

    for element, v in chem_data.items():
        z = v["Z"]
        x = get_node_attributes(element, atom_features="cgcnn")
        if x is not None:
            features[z, :] = x

    # è½¬æ¢ç‰¹å¾
    z = g.ndata.pop("atom_features")
    g.ndata["atomic_number"] = z
    z = z.type(torch.LongTensor).squeeze()
    f = torch.tensor(features[z], dtype=torch.float32)
    g.ndata["atom_features"] = f

    return g, lg, atoms


def main():
    parser = argparse.ArgumentParser(description='Robust Fine-Grained Attention Analysis')
    parser.add_argument('--model_path', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--cif_path', type=str, required=True,
                       help='CIFæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--text', type=str, required=True,
                       help='ææ–™æè¿°æ–‡æœ¬')
    parser.add_argument('--save_dir', type=str, default='./robust_analysis',
                       help='ç»“æœä¿å­˜ç›®å½•')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¡ç®—è®¾å¤‡ (cuda/cpu)')

    args = parser.parse_args()

    print("\n" + "="*80)
    print("ğŸ”¬ Robust Fine-Grained Attention Analysis")
    print("="*80)

    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹...")
    model, config = load_model(args.model_path, device=args.device)

    # åŠ è½½ç»“æ„
    print(f"\nğŸ“‚ åŠ è½½ç»“æ„: {args.cif_path}")
    g, lg, atoms_object = cif_to_graph(args.cif_path)
    print(f"   - åŸå­æ•°: {atoms_object.num_atoms}")
    print(f"   - åŒ–å­¦å¼: {atoms_object.composition.reduced_formula}")
    print(f"   - å…ƒç´ : {', '.join([str(atoms_object.elements[i]) for i in range(atoms_object.num_atoms)])}")

    # è¿è¡Œåˆ†æ
    print(f"\nğŸ” åˆ†ææ–‡æœ¬:")
    text_preview = args.text[:100] + "..." if len(args.text) > 100 else args.text
    print(f'   "{text_preview}"')

    results = run_complete_analysis(
        model=model,
        g=g,
        lg=lg,
        text=args.text,
        atoms_object=atoms_object,
        save_dir=args.save_dir
    )

    if results is None:
        print("\nâŒ åˆ†æå¤±è´¥")
        return

    # æ‰“å°æ‘˜è¦
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("="*80)

    diagnosis = results.get('diagnosis', {})
    print(f"\nè´¨é‡è¯„ä¼°: {diagnosis.get('quality', 'unknown').upper()}")
    print(f"åŸå­å¤šæ ·æ€§åˆ†æ•°: {diagnosis.get('atom_diversity', 0):.4f}")
    print(f"Headå¤šæ ·æ€§åˆ†æ•°: {diagnosis.get('head_diversity', 0):.4f}")
    print(f"å¹³å‡ç†µ: {diagnosis.get('entropy', 0):.4f}")

    if diagnosis.get('issues'):
        print(f"\nâš ï¸  å‘ç°çš„é—®é¢˜:")
        for issue in diagnosis['issues']:
            print(f"   - {issue}")

    if diagnosis.get('recommendations'):
        print(f"\nğŸ’¡ å»ºè®®:")
        for rec in diagnosis['recommendations']:
            print(f"   - {rec}")

    # ç»Ÿè®¡ä¿¡æ¯
    stats = results.get('statistics', {})
    if stats:
        print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   - æ³¨æ„åŠ›å¤´æ•°: {stats.get('num_heads', 'N/A')}")
        print(f"   - åŸå­æ•°: {stats.get('num_atoms', 'N/A')}")
        print(f"   - åºåˆ—é•¿åº¦: {stats.get('seq_len', 'N/A')}")
        print(f"   - å¹³å‡æ³¨æ„åŠ›: {stats.get('mean_attention', 0):.6f}")
        print(f"   - æ³¨æ„åŠ›æ ‡å‡†å·®: {stats.get('std_attention', 0):.6f}")
        print(f"   - ç¨€ç–åº¦: {stats.get('sparsity', 0)*100:.2f}%")

    print(f"\nâœ… åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {args.save_dir}")
    print("="*80 + "\n")


if __name__ == '__main__':
    main()
