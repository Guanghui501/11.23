#!/usr/bin/env python
"""
æ±‡æ€»å’Œå¯è§†åŒ–èåˆä½ç½®å¯¹æ¯”å®éªŒçš„ç»“æœ
"""

import os
import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

sns.set_style("whitegrid")
plt.rcParams['font.size'] = 11


def load_training_history(result_dir):
    """åŠ è½½è®­ç»ƒå†å²"""
    history_file = os.path.join(result_dir, 'training_history.json')
    if not os.path.exists(history_file):
        print(f"âš ï¸  æœªæ‰¾åˆ°è®­ç»ƒå†å²: {history_file}")
        return None

    with open(history_file, 'r') as f:
        history = json.load(f)
    return history


def load_feature_metrics(analysis_dir):
    """åŠ è½½ç‰¹å¾è´¨é‡æŒ‡æ ‡"""
    metrics_file = os.path.join(analysis_dir, 'regression_metrics.csv')
    if not os.path.exists(metrics_file):
        print(f"âš ï¸  æœªæ‰¾åˆ°ç‰¹å¾æŒ‡æ ‡: {metrics_file}")
        return None

    df = pd.read_csv(metrics_file)
    return df


def summarize_experiments(experiments):
    """æ±‡æ€»å®éªŒç»“æœ"""

    print("\n" + "="*60)
    print("  èåˆä½ç½®å¯¹æ¯”å®éªŒç»“æœæ±‡æ€»")
    print("="*60 + "\n")

    summary_data = []

    for exp_name, exp_config in experiments.items():
        result_dir = exp_config['result_dir']
        analysis_dir = exp_config['analysis_dir']

        print(f"ğŸ“Š {exp_name}")
        print("-" * 60)

        # åŠ è½½è®­ç»ƒå†å²
        history = load_training_history(result_dir)
        if history:
            best_test_mae = min(history.get('test_mae', [float('inf')]))
            best_val_mae = min(history.get('val_mae', [float('inf')]))
            final_test_mae = history.get('test_mae', [])[-1] if history.get('test_mae') else None

            print(f"  æœ€ä½³æµ‹è¯•MAE: {best_test_mae:.4f}")
            print(f"  æœ€ä½³éªŒè¯MAE: {best_val_mae:.4f}")
            print(f"  æœ€ç»ˆæµ‹è¯•MAE: {final_test_mae:.4f}")
        else:
            best_test_mae = None
            best_val_mae = None
            final_test_mae = None

        # åŠ è½½ç‰¹å¾æŒ‡æ ‡
        feature_metrics = load_feature_metrics(analysis_dir)
        if feature_metrics is not None:
            # è·å–èåˆç‰¹å¾çš„æŒ‡æ ‡
            fused_row = feature_metrics[feature_metrics['Feature'] == 'fused']
            if not fused_row.empty:
                avg_pearson = fused_row['Avg Pearson Corr'].values[0]
                max_pearson = fused_row['Max Pearson Corr'].values[0]
                print(f"  å¹³å‡Pearsonç›¸å…³æ€§: {avg_pearson:.4f}")
                print(f"  æœ€å¤§Pearsonç›¸å…³æ€§: {max_pearson:.4f}")
            else:
                avg_pearson = None
                max_pearson = None
        else:
            avg_pearson = None
            max_pearson = None

        summary_data.append({
            'Experiment': exp_name,
            'Best Test MAE': best_test_mae,
            'Best Val MAE': best_val_mae,
            'Final Test MAE': final_test_mae,
            'Avg Pearson Corr': avg_pearson,
            'Max Pearson Corr': max_pearson,
            'Fusion Type': exp_config['fusion_type']
        })

        print()

    # åˆ›å»ºæ±‡æ€»è¡¨
    summary_df = pd.DataFrame(summary_data)

    print("\n" + "="*60)
    print("  ç»¼åˆå¯¹æ¯”è¡¨")
    print("="*60 + "\n")
    print(summary_df.to_string(index=False))
    print()

    # ä¿å­˜æ±‡æ€»è¡¨
    summary_df.to_csv('fusion_comparison_summary.csv', index=False)
    print("âœ… æ±‡æ€»è¡¨å·²ä¿å­˜: fusion_comparison_summary.csv\n")

    return summary_df


def plot_comparison(summary_df, output_dir='./'):
    """ç»˜åˆ¶å¯¹æ¯”å›¾"""

    print("ğŸ“ˆ ç”Ÿæˆå¯¹æ¯”å¯è§†åŒ–...")

    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    experiments = summary_df['Experiment'].values
    colors = ['#3498db', '#e74c3c', '#2ecc71']  # è“ã€çº¢ã€ç»¿

    # 1. æµ‹è¯•MAEå¯¹æ¯”
    ax = axes[0, 0]
    best_mae = summary_df['Best Test MAE'].values
    x = range(len(experiments))
    bars = ax.bar(x, best_mae, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
    ax.set_xticks(x)
    ax.set_xticklabels(experiments, rotation=15, ha='right')
    ax.set_ylabel('MAE (eV/atom)', fontweight='bold')
    ax.set_title('æœ€ä½³æµ‹è¯•MAEå¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)
    ax.grid(axis='y', alpha=0.3, linestyle='--')

    # æ ‡æ³¨æ•°å€¼
    for i, v in enumerate(best_mae):
        ax.text(i, v + 0.002, f'{v:.4f}', ha='center', va='bottom',
                fontsize=10, fontweight='bold')

    # æ ‡æ³¨æœ€ä½³
    best_idx = np.argmin(best_mae)
    ax.scatter(best_idx, best_mae[best_idx], s=200, marker='*',
              color='gold', edgecolor='black', linewidth=2, zorder=10)

    # 2. æ”¶æ•›æ›²çº¿å¯¹æ¯”
    ax = axes[0, 1]
    for i, (exp_name, exp_config) in enumerate(experiments_config.items()):
        history = load_training_history(exp_config['result_dir'])
        if history and 'test_mae' in history:
            epochs = range(1, len(history['test_mae']) + 1)
            ax.plot(epochs, history['test_mae'], label=exp_name,
                   color=colors[i], linewidth=2, alpha=0.8)

    ax.set_xlabel('Epoch', fontweight='bold')
    ax.set_ylabel('Test MAE (eV/atom)', fontweight='bold')
    ax.set_title('è®­ç»ƒæ”¶æ•›æ›²çº¿å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)
    ax.legend(loc='upper right', framealpha=0.9)
    ax.grid(alpha=0.3, linestyle='--')

    # 3. Pearsonç›¸å…³æ€§å¯¹æ¯”
    ax = axes[1, 0]
    avg_pearson = summary_df['Avg Pearson Corr'].values
    max_pearson = summary_df['Max Pearson Corr'].values

    x_pos = np.arange(len(experiments))
    width = 0.35

    bars1 = ax.bar(x_pos - width/2, avg_pearson, width, label='å¹³å‡ç›¸å…³æ€§',
                   color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5)
    bars2 = ax.bar(x_pos + width/2, max_pearson, width, label='æœ€å¤§ç›¸å…³æ€§',
                   color='#e74c3c', alpha=0.8, edgecolor='black', linewidth=1.5)

    ax.set_xticks(x_pos)
    ax.set_xticklabels(experiments, rotation=15, ha='right')
    ax.set_ylabel('Pearsonç›¸å…³ç³»æ•°', fontweight='bold')
    ax.set_title('ç‰¹å¾-ç›®æ ‡ç›¸å…³æ€§å¯¹æ¯”', fontsize=14, fontweight='bold', pad=20)
    ax.legend(loc='upper left', framealpha=0.9)
    ax.grid(axis='y', alpha=0.3, linestyle='--')
    ax.axhline(y=0.5, color='orange', linestyle='--', linewidth=1.5,
              alpha=0.6, label='å¼ºç›¸å…³(0.5)')

    # æ ‡æ³¨æ•°å€¼
    for i, (v1, v2) in enumerate(zip(avg_pearson, max_pearson)):
        ax.text(i - width/2, v1 + 0.01, f'{v1:.3f}', ha='center', va='bottom', fontsize=9)
        ax.text(i + width/2, v2 + 0.01, f'{v2:.3f}', ha='center', va='bottom', fontsize=9)

    # 4. æ€§èƒ½é›·è¾¾å›¾
    ax = axes[1, 1]
    ax.axis('off')

    # åˆ›å»ºæåæ ‡å­å›¾
    ax_polar = fig.add_subplot(2, 2, 4, projection='polar')

    # å½’ä¸€åŒ–æŒ‡æ ‡ (è¶Šå°è¶Šå¥½çš„MAEéœ€è¦åè½¬)
    mae_normalized = 1 - (best_mae - best_mae.min()) / (best_mae.max() - best_mae.min() + 1e-10)
    pearson_normalized = (avg_pearson - avg_pearson.min()) / (avg_pearson.max() - avg_pearson.min() + 1e-10)

    # é›·è¾¾å›¾æ•°æ®
    categories = ['ä½è¯¯å·®', 'é«˜ç›¸å…³æ€§', 'ç¨³å®šæ€§']
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆ

    for i, exp_name in enumerate(experiments):
        values = [
            mae_normalized[i],  # ä½è¯¯å·®
            pearson_normalized[i],  # é«˜ç›¸å…³æ€§
            0.8  # ç¨³å®šæ€§(å ä½)
        ]
        values += values[:1]  # é—­åˆ

        ax_polar.plot(angles, values, 'o-', linewidth=2, label=exp_name,
                     color=colors[i], alpha=0.8)
        ax_polar.fill(angles, values, alpha=0.15, color=colors[i])

    ax_polar.set_xticks(angles[:-1])
    ax_polar.set_xticklabels(categories, fontsize=11)
    ax_polar.set_ylim(0, 1)
    ax_polar.set_title('ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontsize=14, fontweight='bold',
                      pad=20, y=1.08)
    ax_polar.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), framealpha=0.9)
    ax_polar.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(output_dir, 'fusion_comparison_summary.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}\n")
    plt.close()


def generate_report(summary_df, output_file='fusion_comparison_report.md'):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""

    print("ğŸ“ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")

    report = []
    report.append("# èåˆä½ç½®å¯¹æ¯”å®éªŒæŠ¥å‘Š\n")
    report.append("## å®éªŒè®¾è®¡\n")
    report.append("æœ¬å®éªŒå¯¹æ¯”äº†ä¸‰ç§ä¸åŒçš„æ–‡æœ¬-å›¾èåˆç­–ç•¥:\n")
    report.append("1. **ALIGNNå±‚èåˆ**: åœ¨ALIGNNç¼–ç æ—©æœŸæ³¨å…¥æ–‡æœ¬ä¿¡æ¯(ä¸­é—´èåˆ)\n")
    report.append("2. **GCNå±‚èåˆ**: åœ¨GCNå±‚ä¹‹åè¿›è¡Œç»†ç²’åº¦åŸå­-è¯å…ƒæ³¨æ„åŠ›\n")
    report.append("3. **å±‚æ¬¡åŒ–èåˆ**: ç»“åˆALIGNNã€GCNå’Œå…¨å±€ä¸‰ä¸ªå±‚æ¬¡çš„èåˆ\n\n")

    report.append("## å®éªŒç»“æœ\n\n")
    report.append("### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”\n\n")
    report.append(summary_df.to_markdown(index=False))
    report.append("\n\n")

    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_idx = summary_df['Best Test MAE'].idxmin()
    best_exp = summary_df.loc[best_idx]

    report.append("### å…³é”®å‘ç°\n\n")
    report.append(f"#### ğŸ† æœ€ä½³æ¨¡å‹: {best_exp['Experiment']}\n\n")
    report.append(f"- **æœ€ä½³æµ‹è¯•MAE**: {best_exp['Best Test MAE']:.4f} eV/atom\n")
    report.append(f"- **å¹³å‡Pearsonç›¸å…³æ€§**: {best_exp['Avg Pearson Corr']:.4f}\n")
    report.append(f"- **èåˆç±»å‹**: {best_exp['Fusion Type']}\n\n")

    # åˆ†æå„ä¸ªæ¨¡å‹çš„ä¼˜åŠ£
    report.append("#### ğŸ“Š å„æ¨¡å‹åˆ†æ\n\n")

    for idx, row in summary_df.iterrows():
        report.append(f"**{row['Experiment']}**:\n")

        if row['Fusion Type'] == 'ALIGNN Early Fusion':
            report.append("- ä¼˜åŠ¿: æ–‡æœ¬ä¿¡æ¯ä¼ æ’­è·ç¦»æœ€é•¿,å…¨å±€è¯­ä¹‰æŒ‡å¯¼å……åˆ†\n")
            report.append("- åŠ£åŠ¿: å¯èƒ½å¹²æ‰°åº•å±‚å‡ ä½•ç‰¹å¾æå–\n")
        elif row['Fusion Type'] == 'GCN Late Fusion':
            report.append("- ä¼˜åŠ¿: å‡ ä½•ç‰¹å¾å·²å……åˆ†æå–,ç»†ç²’åº¦å¯¹é½ç²¾å‡†\n")
            report.append("- åŠ£åŠ¿: æ–‡æœ¬ä¿¡æ¯ä¼ æ’­æ·±åº¦å—é™\n")
        else:
            report.append("- ä¼˜åŠ¿: å¤šå±‚æ¬¡èåˆ,å……åˆ†åˆ©ç”¨æ–‡æœ¬çš„å…¨å±€å’Œå±€éƒ¨ä¿¡æ¯\n")
            report.append("- åŠ£åŠ¿: è®¡ç®—æˆæœ¬è¾ƒé«˜\n")

        report.append(f"- æµ‹è¯•MAE: {row['Best Test MAE']:.4f}\n")
        report.append(f"- ç‰¹å¾ç›¸å…³æ€§: {row['Avg Pearson Corr']:.4f}\n\n")

    report.append("## ç»“è®ºä¸å»ºè®®\n\n")

    # æ ¹æ®ç»“æœç»™å‡ºå»ºè®®
    mae_values = summary_df['Best Test MAE'].values
    mae_range = mae_values.max() - mae_values.min()

    if mae_range < 0.01:  # å·®å¼‚å¾ˆå°
        report.append("### ğŸ“Œ å®éªŒç»“è®º\n\n")
        report.append("ä¸‰ç§èåˆç­–ç•¥çš„æ€§èƒ½å·®å¼‚è¾ƒå°(MAEå·®å¼‚<0.01),è¯´æ˜:\n")
        report.append("1. èåˆä½ç½®å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“æœ‰é™\n")
        report.append("2. æ–‡æœ¬å’Œå›¾ç»“æ„ä¿¡æ¯å·²ç»è¾ƒå¥½åœ°äº’è¡¥\n")
        report.append("3. å¯ä»¥æ ¹æ®è®¡ç®—æ•ˆç‡é€‰æ‹©æ›´ç®€å•çš„èåˆç­–ç•¥\n\n")
    else:  # å·®å¼‚æ˜æ˜¾
        report.append("### ğŸ“Œ å®éªŒç»“è®º\n\n")
        report.append(f"èåˆä½ç½®å¯¹æ€§èƒ½æœ‰æ˜æ˜¾å½±å“(MAEå·®å¼‚={mae_range:.4f}):\n")
        report.append(f"1. **æœ€ä½³ç­–ç•¥**: {best_exp['Experiment']}\n")
        report.append(f"2. **æ€§èƒ½æå‡**: ç›¸æ¯”æœ€å·®æ¨¡å‹æå‡äº† {(mae_range/mae_values.max()*100):.1f}%\n")
        report.append("3. **å»ºè®®**: æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„èåˆä½ç½®\n\n")

    report.append("### ğŸ¯ åº”ç”¨å»ºè®®\n\n")
    report.append("- **å…¨å±€å±æ€§é¢„æµ‹** (å¦‚å½¢æˆèƒ½ã€å¸¦éš™): ä¼˜å…ˆä½¿ç”¨ALIGNNå±‚èåˆæˆ–å±‚æ¬¡åŒ–èåˆ\n")
    report.append("- **å±€éƒ¨æ€§è´¨é¢„æµ‹** (å¦‚åŸå­åŠ›ã€å±€éƒ¨ç£çŸ©): ä¼˜å…ˆä½¿ç”¨GCNå±‚èåˆ\n")
    report.append("- **è®¡ç®—èµ„æºå—é™**: å•ç‹¬ä½¿ç”¨ALIGNNæˆ–GCNèåˆ\n")
    report.append("- **è¿½æ±‚æœ€ä½³æ€§èƒ½**: ä½¿ç”¨å±‚æ¬¡åŒ–èåˆ\n\n")

    report.append("## å¯è§†åŒ–ç»“æœ\n\n")
    report.append("è¯¦ç»†çš„å¯è§†åŒ–ç»“æœè¯·æŸ¥çœ‹:\n")
    report.append("- `fusion_comparison_summary.png` - ç»¼åˆå¯¹æ¯”å›¾\n")
    report.append("- `analysis/*/tsne_comparison.png` - ç‰¹å¾åˆ†å¸ƒt-SNEå¯è§†åŒ–\n")
    report.append("- `analysis/*/regression_metrics_comparison.png` - å›å½’æŒ‡æ ‡å¯¹æ¯”\n\n")

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(''.join(report))

    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {output_file}\n")


# å®éªŒé…ç½®
experiments_config = {
    'ALIGNNå±‚èåˆ': {
        'result_dir': 'results/fusion_at_alignn',
        'analysis_dir': 'analysis/fusion_at_alignn',
        'fusion_type': 'ALIGNN Early Fusion'
    },
    'GCNå±‚èåˆ': {
        'result_dir': 'results/fusion_at_gcn',
        'analysis_dir': 'analysis/fusion_at_gcn',
        'fusion_type': 'GCN Late Fusion'
    },
    'å±‚æ¬¡åŒ–èåˆ': {
        'result_dir': 'results/fusion_hierarchical',
        'analysis_dir': 'analysis/fusion_hierarchical',
        'fusion_type': 'Hierarchical Fusion'
    }
}


if __name__ == '__main__':
    # æ±‡æ€»å®éªŒç»“æœ
    summary_df = summarize_experiments(experiments_config)

    # ç”Ÿæˆå¯¹æ¯”å›¾
    plot_comparison(summary_df, output_dir='./')

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(summary_df)

    print("\n" + "="*60)
    print("  ğŸ‰ åˆ†æå®Œæˆ!")
    print("="*60)
    print("\nç”Ÿæˆçš„æ–‡ä»¶:")
    print("  - fusion_comparison_summary.csv (æ±‡æ€»æ•°æ®)")
    print("  - fusion_comparison_summary.png (å¯¹æ¯”å›¾)")
    print("  - fusion_comparison_report.md (è¯¦ç»†æŠ¥å‘Š)")
    print()
