#!/usr/bin/env python
"""
æ¨¡å‹æ€§èƒ½å¯¹æ¯”è„šæœ¬
ç”¨äºåˆ†æä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å·®å¼‚ï¼Œç»“åˆCKAç›¸ä¼¼åº¦åˆ¤æ–­èåˆæœºåˆ¶çš„æœ‰æ•ˆæ€§
"""

import os
import argparse
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr, spearmanr

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 12

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
import sys
sys.path.insert(0, os.path.dirname(__file__))
from models.alignn import ALIGNN
from train_with_cross_modal_attention import load_dataset, get_dataset_paths
from data import get_train_val_loaders


def load_model(path, device):
    """åŠ è½½æ¨¡å‹"""
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {path}")
    ckpt = torch.load(path, map_location=device, weights_only=False)
    config = ckpt.get('config') or ckpt.get('model_config')
    model = ALIGNN(config)
    model.load_state_dict(ckpt['model'])
    model.to(device)
    model.eval()
    print(f"   é…ç½®: ä¸­æœŸèåˆ={model.use_middle_fusion}, "
          f"ç»†ç²’åº¦æ³¨æ„åŠ›={model.use_fine_grained_attention}, "
          f"å…¨å±€æ³¨æ„åŠ›={model.use_cross_modal_attention}")
    return model


def get_predictions(model, loader, device, max_samples=None):
    """
    è·å–æ¨¡å‹é¢„æµ‹

    Returns:
        predictions: é¢„æµ‹å€¼
        targets: çœŸå®å€¼
    """
    print("ğŸ”„ è·å–æ¨¡å‹é¢„æµ‹...")

    predictions = []
    targets = []

    sample_count = 0

    with torch.no_grad():
        for batch in tqdm(loader, desc="é¢„æµ‹ä¸­"):
            if len(batch) == 3:
                g, text, y = batch
                lg = None
            elif len(batch) == 4:
                g, lg, text, y = batch
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„batchæ ¼å¼: {len(batch)}ä¸ªå…ƒç´ ")

            g = g.to(device)
            if lg is not None:
                lg = lg.to(device)

            # å¤„ç†text
            if isinstance(text, dict):
                text = {k: v.to(device) for k, v in text.items()}
            elif isinstance(text, (list, tuple)):
                pass
            elif torch.is_tensor(text):
                text = text.to(device)

            # æ„å»ºæ¨¡å‹è¾“å…¥
            if lg is not None:
                model_input = (g, lg, text)
            else:
                model_input = (g, text)

            # è·å–é¢„æµ‹
            out = model(model_input)
            pred = out if torch.is_tensor(out) else out.get('prediction', out.get('out'))

            predictions.append(pred.cpu().numpy())
            targets.append(y.cpu().numpy())

            sample_count += y.size(0)
            if max_samples and sample_count >= max_samples:
                break

    predictions = np.concatenate(predictions).flatten()
    targets = np.concatenate(targets).flatten()

    print(f"âœ… é¢„æµ‹å®Œæˆ! æ ·æœ¬æ•°: {len(targets)}")

    return predictions, targets


def compute_metrics(predictions, targets):
    """è®¡ç®—æ€§èƒ½æŒ‡æ ‡"""
    mae = mean_absolute_error(targets, predictions)
    rmse = np.sqrt(mean_squared_error(targets, predictions))
    r2 = r2_score(targets, predictions)
    pearson_corr, _ = pearsonr(targets, predictions)
    spearman_corr, _ = spearmanr(targets, predictions)

    return {
        'MAE': mae,
        'RMSE': rmse,
        'R2': r2,
        'Pearson': pearson_corr,
        'Spearman': spearman_corr
    }


def visualize_predictions(pred1, pred2, targets, model1_name, model2_name, save_dir):
    """å¯è§†åŒ–é¢„æµ‹ç»“æœå¯¹æ¯”"""
    print("\nğŸ“Š ç”Ÿæˆé¢„æµ‹å¯¹æ¯”å¯è§†åŒ–...")

    fig, axes = plt.subplots(2, 3, figsize=(18, 12))

    # è®¡ç®—æŒ‡æ ‡
    metrics1 = compute_metrics(pred1, targets)
    metrics2 = compute_metrics(pred2, targets)

    # 1. Model 1 é¢„æµ‹æ•£ç‚¹å›¾
    ax = axes[0, 0]
    ax.scatter(targets, pred1, alpha=0.5, s=20)
    ax.plot([targets.min(), targets.max()], [targets.min(), targets.max()],
            'r--', lw=2, label='Perfect Prediction')
    ax.set_xlabel('True Values', fontweight='bold')
    ax.set_ylabel('Predictions', fontweight='bold')
    ax.set_title(f'{model1_name}\nMAE={metrics1["MAE"]:.4f}, RÂ²={metrics1["R2"]:.4f}',
                fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 2. Model 2 é¢„æµ‹æ•£ç‚¹å›¾
    ax = axes[0, 1]
    ax.scatter(targets, pred2, alpha=0.5, s=20, color='orange')
    ax.plot([targets.min(), targets.max()], [targets.min(), targets.max()],
            'r--', lw=2, label='Perfect Prediction')
    ax.set_xlabel('True Values', fontweight='bold')
    ax.set_ylabel('Predictions', fontweight='bold')
    ax.set_title(f'{model2_name}\nMAE={metrics2["MAE"]:.4f}, RÂ²={metrics2["R2"]:.4f}',
                fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 3. é¢„æµ‹å·®å¼‚æ•£ç‚¹å›¾
    ax = axes[0, 2]
    pred_diff = np.abs(pred1 - pred2)
    scatter = ax.scatter(targets, pred_diff, c=pred_diff, cmap='coolwarm',
                        alpha=0.6, s=20)
    ax.set_xlabel('True Values', fontweight='bold')
    ax.set_ylabel('|Pred1 - Pred2|', fontweight='bold')
    ax.set_title(f'Prediction Difference\nMean={pred_diff.mean():.4f}, Max={pred_diff.max():.4f}',
                fontweight='bold')
    plt.colorbar(scatter, ax=ax, label='Difference')
    ax.grid(True, alpha=0.3)

    # 4. è¯¯å·®åˆ†å¸ƒå¯¹æ¯”
    ax = axes[1, 0]
    error1 = pred1 - targets
    error2 = pred2 - targets
    ax.hist(error1, bins=50, alpha=0.5, label=model1_name, color='blue')
    ax.hist(error2, bins=50, alpha=0.5, label=model2_name, color='orange')
    ax.set_xlabel('Prediction Error', fontweight='bold')
    ax.set_ylabel('Frequency', fontweight='bold')
    ax.set_title('Error Distribution Comparison', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    # 5. æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾
    ax = axes[1, 1]
    metrics_names = ['MAE', 'RMSE', 'R2', 'Pearson', 'Spearman']
    x = np.arange(len(metrics_names))
    width = 0.35

    values1 = [metrics1[m] for m in metrics_names]
    values2 = [metrics2[m] for m in metrics_names]

    bars1 = ax.bar(x - width/2, values1, width, label=model1_name, alpha=0.8)
    bars2 = ax.bar(x + width/2, values2, width, label=model2_name, alpha=0.8)

    ax.set_ylabel('Value', fontweight='bold')
    ax.set_title('Metrics Comparison', fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels(metrics_names, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    # æ ‡æ³¨æ•°å€¼
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.3f}', ha='center', va='bottom', fontsize=8)

    # 6. é¢„æµ‹ä¸€è‡´æ€§æ•£ç‚¹å›¾
    ax = axes[1, 2]
    ax.scatter(pred1, pred2, alpha=0.5, s=20)
    ax.plot([pred1.min(), pred1.max()], [pred1.min(), pred1.max()],
            'r--', lw=2, label='Perfect Agreement')
    ax.set_xlabel(f'{model1_name} Predictions', fontweight='bold')
    ax.set_ylabel(f'{model2_name} Predictions', fontweight='bold')
    corr = np.corrcoef(pred1, pred2)[0, 1]
    ax.set_title(f'Prediction Agreement\nCorrelation={corr:.4f}', fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)

    plt.tight_layout()
    save_path = os.path.join(save_dir, 'performance_comparison.png')
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜: {save_path}")
    plt.close()

    return metrics1, metrics2


def generate_performance_report(metrics1, metrics2, pred1, pred2, targets,
                                model1_name, model2_name, save_dir):
    """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
    print("\nğŸ“ ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š...")

    report_lines = []
    report_lines.append("=" * 80)
    report_lines.append("Model Performance Comparison Report")
    report_lines.append(f"Model 1: {model1_name}")
    report_lines.append(f"Model 2: {model2_name}")
    report_lines.append("=" * 80)
    report_lines.append("")

    # 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”
    report_lines.append("ğŸ“Š Performance Metrics Comparison:")
    report_lines.append("")
    report_lines.append(f"{'Metric':<20} {model1_name:<15} {model2_name:<15} {'Difference':<15} {'Change %':<15}")
    report_lines.append("-" * 80)

    for metric in ['MAE', 'RMSE', 'R2', 'Pearson', 'Spearman']:
        val1 = metrics1[metric]
        val2 = metrics2[metric]
        diff = val2 - val1

        # å¯¹äºMAEå’ŒRMSEï¼Œè¶Šå°è¶Šå¥½ï¼›å¯¹äºå…¶ä»–æŒ‡æ ‡ï¼Œè¶Šå¤§è¶Šå¥½
        if metric in ['MAE', 'RMSE']:
            change_pct = (diff / val1) * 100 if val1 != 0 else 0
            better = "âœ“" if diff < 0 else "âœ—"
        else:
            change_pct = (diff / val1) * 100 if val1 != 0 else 0
            better = "âœ“" if diff > 0 else "âœ—"

        report_lines.append(f"{metric:<20} {val1:<15.4f} {val2:<15.4f} "
                          f"{diff:+.4f} ({change_pct:+.2f}%) {better}")

    report_lines.append("")

    # 2. é¢„æµ‹å·®å¼‚åˆ†æ
    pred_diff = np.abs(pred1 - pred2)
    pred_corr = np.corrcoef(pred1, pred2)[0, 1]

    report_lines.append("ğŸ” Prediction Difference Analysis:")
    report_lines.append(f"  â€¢ Prediction Correlation: {pred_corr:.4f}")
    report_lines.append(f"  â€¢ Mean Absolute Difference: {pred_diff.mean():.4f}")
    report_lines.append(f"  â€¢ Max Absolute Difference: {pred_diff.max():.4f}")
    report_lines.append(f"  â€¢ Std Absolute Difference: {pred_diff.std():.4f}")
    report_lines.append("")

    # è®¡ç®—é¢„æµ‹å·®å¼‚çš„ç™¾åˆ†ä½æ•°
    percentiles = [50, 75, 90, 95, 99]
    report_lines.append("  Prediction Difference Percentiles:")
    for p in percentiles:
        val = np.percentile(pred_diff, p)
        report_lines.append(f"    {p}th percentile: {val:.4f}")
    report_lines.append("")

    # 3. å…³é”®å‘ç°å’Œè§£é‡Š
    report_lines.append("ğŸ’¡ Key Findings:")
    report_lines.append("")

    # åˆ¤æ–­æ€§èƒ½å·®å¼‚æ˜¯å¦æ˜¾è‘—
    mae_diff_pct = abs((metrics2['MAE'] - metrics1['MAE']) / metrics1['MAE']) * 100
    r2_diff_pct = abs((metrics2['R2'] - metrics1['R2']) / metrics1['R2']) * 100

    if mae_diff_pct < 1 and r2_diff_pct < 1:
        report_lines.append("  ğŸ¯ ç»“è®º: ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½å‡ ä¹ç›¸åŒ")
        report_lines.append("")
        report_lines.append("  å¯èƒ½çš„è§£é‡Š:")
        report_lines.append("  1. èåˆæœºåˆ¶æ”¹å˜äº†ä¸­é—´è¡¨ç¤ºï¼Œä½†æœ€ç»ˆæ”¶æ•›åˆ°ç›¸ä¼¼çš„é¢„æµ‹")
        report_lines.append("  2. æ¨¡å‹æ¶æ„çš„å…¶ä»–éƒ¨åˆ†ï¼ˆå¦‚è¾“å‡ºå±‚ï¼‰ä¸»å¯¼äº†æœ€ç»ˆé¢„æµ‹")
        report_lines.append("  3. æ•°æ®é›†å¯èƒ½ä¸éœ€è¦å¤æ‚çš„èåˆæœºåˆ¶å³å¯è¾¾åˆ°æ€§èƒ½ä¸Šé™")
        report_lines.append("")
        report_lines.append("  âš ï¸  è­¦å‘Š: CKAç›¸ä¼¼åº¦é«˜ + æ€§èƒ½ç›¸åŒ = èåˆæœºåˆ¶å¯èƒ½æœªå……åˆ†åˆ©ç”¨")
        report_lines.append("")
        report_lines.append("  å»ºè®®:")
        report_lines.append("  â€¢ æ£€æŸ¥èåˆæœºåˆ¶æ˜¯å¦çœŸçš„åœ¨èµ·ä½œç”¨ï¼ˆå¯èƒ½è¢«åç»­å±‚æŠµæ¶ˆäº†ï¼‰")
        report_lines.append("  â€¢ å°è¯•æ›´å¼ºçš„èåˆæœºåˆ¶æˆ–æ›´æ—©çš„èåˆä½ç½®")
        report_lines.append("  â€¢ è€ƒè™‘ä½¿ç”¨æ›´å…·æŒ‘æˆ˜æ€§çš„æ•°æ®é›†æ¥éªŒè¯èåˆæ•ˆæœ")
        report_lines.append("  â€¢ åˆ†ætext_fineé˜¶æ®µçš„å·®å¼‚æ˜¯å¦è¢«åç»­å±‚"æŠ¹å¹³"äº†")

    elif mae_diff_pct < 5 and r2_diff_pct < 5:
        report_lines.append("  ğŸ¯ ç»“è®º: ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æœ‰è½»å¾®å·®å¼‚")
        report_lines.append("")

        if metrics2['MAE'] < metrics1['MAE']:
            report_lines.append(f"  âœ“ {model2_name} æ€§èƒ½æ›´å¥½ (MAEé™ä½ {mae_diff_pct:.2f}%)")
            report_lines.append("")
            report_lines.append("  è§£é‡Š: èåˆæœºåˆ¶å¸¦æ¥äº†å°å¹…ä½†ä¸€è‡´çš„æ€§èƒ½æå‡")
            report_lines.append("  â€¢ CKAç›¸ä¼¼åº¦é«˜è¯´æ˜æœ€ç»ˆè¡¨ç¤ºç›¸è¿‘")
            report_lines.append("  â€¢ ä½†å¾®å°çš„å·®å¼‚è¶³ä»¥æ”¹å–„é¢„æµ‹å‡†ç¡®åº¦")
            report_lines.append("  â€¢ è¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¼˜åŒ–ç»“æœ")
        else:
            report_lines.append(f"  âœ— {model2_name} æ€§èƒ½åè€Œä¸‹é™ (MAEå¢åŠ  {mae_diff_pct:.2f}%)")
            report_lines.append("")
            report_lines.append("  âš ï¸  è­¦å‘Š: èåˆæœºåˆ¶æœªèƒ½å¸¦æ¥æ€§èƒ½æå‡")
            report_lines.append("  å¯èƒ½åŸå› :")
            report_lines.append("  â€¢ èåˆå¼•å…¥äº†å™ªå£°æˆ–è¿‡æ‹Ÿåˆ")
            report_lines.append("  â€¢ èåˆä½ç½®æˆ–å¼ºåº¦ä¸åˆé€‚")
            report_lines.append("  â€¢ éœ€è¦è°ƒæ•´è¶…å‚æ•°æˆ–è®­ç»ƒç­–ç•¥")

    else:
        report_lines.append("  ğŸ¯ ç»“è®º: ä¸¤ä¸ªæ¨¡å‹çš„é¢„æµ‹æ€§èƒ½æœ‰æ˜¾è‘—å·®å¼‚")
        report_lines.append("")

        if metrics2['MAE'] < metrics1['MAE']:
            report_lines.append(f"  âœ“âœ“ {model2_name} æ€§èƒ½æ˜æ˜¾æ›´å¥½ (MAEé™ä½ {mae_diff_pct:.2f}%)")
            report_lines.append("")
            report_lines.append("  è§£é‡Š: èåˆæœºåˆ¶æœ‰æ•ˆæ”¹å–„äº†æ¨¡å‹æ€§èƒ½")
            report_lines.append("  â€¢ å°½ç®¡CKAç›¸ä¼¼åº¦é«˜ï¼Œä½†å…³é”®çš„å·®å¼‚è¶³ä»¥äº§ç”Ÿæ˜¾è‘—æ•ˆæœ")
            report_lines.append("  â€¢ èåˆæœºåˆ¶æˆåŠŸæ•è·äº†æœ‰ç”¨çš„è·¨æ¨¡æ€ä¿¡æ¯")
            report_lines.append("  â€¢ è¿™æ˜¯ç†æƒ³çš„èåˆæ•ˆæœ")
        else:
            report_lines.append(f"  âœ—âœ— {model2_name} æ€§èƒ½æ˜æ˜¾ä¸‹é™ (MAEå¢åŠ  {mae_diff_pct:.2f}%)")
            report_lines.append("")
            report_lines.append("  âš ï¸âš ï¸  ä¸¥é‡è­¦å‘Š: èåˆæœºåˆ¶ä¸¥é‡æŸå®³äº†æ€§èƒ½")
            report_lines.append("  éœ€è¦ç«‹å³æ£€æŸ¥:")
            report_lines.append("  â€¢ æ¨¡å‹è®­ç»ƒæ˜¯å¦æ”¶æ•›")
            report_lines.append("  â€¢ èåˆæœºåˆ¶æ˜¯å¦æœ‰bug")
            report_lines.append("  â€¢ è¶…å‚æ•°æ˜¯å¦åˆç†")

    report_lines.append("")
    report_lines.append("=" * 80)

    # ä¿å­˜æŠ¥å‘Š
    report_text = "\n".join(report_lines)
    save_path = os.path.join(save_dir, 'performance_report.txt')
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write(report_text)

    print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜: {save_path}")
    print("\n" + report_text)

    # ä¿å­˜CSV
    df = pd.DataFrame({
        'Metric': list(metrics1.keys()),
        model1_name: list(metrics1.values()),
        model2_name: list(metrics2.values()),
        'Difference': [metrics2[k] - metrics1[k] for k in metrics1.keys()],
        'Change_%': [((metrics2[k] - metrics1[k]) / metrics1[k] * 100) if metrics1[k] != 0 else 0
                     for k in metrics1.keys()]
    })
    csv_path = os.path.join(save_dir, 'performance_metrics.csv')
    df.to_csv(csv_path, index=False)
    print(f"âœ… æ€§èƒ½æŒ‡æ ‡CSVå·²ä¿å­˜: {csv_path}")


def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹æ€§èƒ½å¯¹æ¯”åˆ†æ')
    parser.add_argument('--ckpt_model1', type=str, required=True,
                       help='æ¨¡å‹1çš„checkpointè·¯å¾„')
    parser.add_argument('--ckpt_model2', type=str, required=True,
                       help='æ¨¡å‹2çš„checkpointè·¯å¾„')
    parser.add_argument('--model1_name', type=str, default='Model 1',
                       help='æ¨¡å‹1çš„åç§°')
    parser.add_argument('--model2_name', type=str, default='Model 2',
                       help='æ¨¡å‹2çš„åç§°')
    parser.add_argument('--dataset', type=str, required=True,
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--property', type=str, required=True,
                       help='ç›®æ ‡å±æ€§')
    parser.add_argument('--root_dir', type=str,
                       default='/public/home/ghzhang/crysmmnet-main/dataset',
                       help='æ•°æ®é›†æ ¹ç›®å½•')
    parser.add_argument('--batch_size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--max_samples', type=int, default=500,
                       help='æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--save_dir', type=str, default='./performance_comparison',
                       help='ç»“æœä¿å­˜ç›®å½•')
    args = parser.parse_args()

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(args.save_dir, exist_ok=True)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}\n")

    # åŠ è½½æ¨¡å‹
    print("=" * 80)
    model1 = load_model(args.ckpt_model1, device)
    print("=" * 80)
    model2 = load_model(args.ckpt_model2, device)
    print("=" * 80)

    # åŠ è½½æ•°æ®é›†
    print(f"\nğŸ”„ åŠ è½½æ•°æ®é›†: {args.dataset} - {args.property}")

    try:
        cif_dir, id_prop_file = get_dataset_paths(args.root_dir, args.dataset, args.property)
        df = load_dataset(cif_dir, id_prop_file, args.dataset, args.property)
        print(f"âœ… åŠ è½½æ•°æ®é›†: {len(df)} æ ·æœ¬")

        if args.max_samples and len(df) > args.max_samples:
            print(f"âš ï¸  æ•°æ®é›†è¿‡å¤§ï¼Œéšæœºé‡‡æ · {args.max_samples} æ ·æœ¬")
            import random
            random.seed(42)
            df = random.sample(df, args.max_samples)

        # è·å–é…ç½®
        config = model1.config if hasattr(model1, 'config') else None
        if config is None:
            ckpt = torch.load(args.ckpt_model1, map_location='cpu', weights_only=False)
            config = ckpt.get('config') or ckpt.get('model_config')

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader, _ = get_train_val_loaders(
            dataset='user_data',
            dataset_array=df,
            target='target',
            n_train=None,
            n_val=None,
            n_test=None,
            train_ratio=0.8,
            val_ratio=0.1,
            test_ratio=0.1,
            batch_size=args.batch_size,
            atom_features=config.atom_features if hasattr(config, 'atom_features') else 'cgcnn',
            neighbor_strategy='k-nearest',
            line_graph=config.line_graph if hasattr(config, 'line_graph') else True,
            split_seed=42,
            workers=0,
            pin_memory=False,
            save_dataloader=False,
            filename='temp_performance',
            id_tag='jid',
            use_canonize=True,
            cutoff=8.0,
            max_neighbors=12,
            output_dir=args.save_dir
        )

        print(f"âœ… æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

    except Exception as e:
        print(f"âŒ åŠ è½½æ•°æ®é›†å¤±è´¥: {e}")
        raise

    # è·å–æ¨¡å‹1çš„é¢„æµ‹
    print("\n" + "=" * 80)
    print(f"è·å– {args.model1_name} çš„é¢„æµ‹")
    print("=" * 80)
    pred1, targets = get_predictions(model1, test_loader, device, max_samples=args.max_samples)

    # è·å–æ¨¡å‹2çš„é¢„æµ‹
    print("\n" + "=" * 80)
    print(f"è·å– {args.model2_name} çš„é¢„æµ‹")
    print("=" * 80)
    pred2, _ = get_predictions(model2, test_loader, device, max_samples=args.max_samples)

    # å¯è§†åŒ–å’Œåˆ†æ
    print("\n" + "=" * 80)
    metrics1, metrics2 = visualize_predictions(
        pred1, pred2, targets,
        args.model1_name, args.model2_name,
        args.save_dir
    )

    # ç”ŸæˆæŠ¥å‘Š
    generate_performance_report(
        metrics1, metrics2, pred1, pred2, targets,
        args.model1_name, args.model2_name,
        args.save_dir
    )

    print(f"\nğŸ‰ åˆ†æå®Œæˆ! ç»“æœä¿å­˜åœ¨: {args.save_dir}")


if __name__ == '__main__':
    main()
