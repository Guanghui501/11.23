#!/usr/bin/env python3
"""
è¯Šæ–­è„šæœ¬ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦è¾“å‡ºä¸åŒåŸå­çš„ä¸åŒæ³¨æ„åŠ›æ¨¡å¼

å°†æ­¤ä»£ç æ·»åŠ åˆ°æ‚¨çš„ demo_fine_grained_attention.py ä¸­ï¼Œ
åœ¨ç¬¬ 166 è¡Œ (æå– fine-grained attention ä¹‹å) æ·»åŠ ã€‚
"""

import numpy as np
import torch

def diagnose_fine_grained_attention(fg_attn, elements=None):
    """
    è¯Šæ–­ fine-grained attention æ˜¯å¦ä¸ºæ‰€æœ‰åŸå­è¾“å‡ºç›¸åŒçš„æ¨¡å¼

    Args:
        fg_attn: fine-grained attention weights dict from model
        elements: list of element symbols (optional)
    """

    print("\n" + "="*80)
    print("ğŸ”¬ Fine-Grained Attention è¯Šæ–­")
    print("="*80)

    if fg_attn is None or 'atom_to_text' not in fg_attn:
        print("âŒ æ²¡æœ‰æ‰¾åˆ° fine-grained attention weights!")
        return

    atom_to_text = fg_attn['atom_to_text']  # [batch, heads, num_atoms, seq_len]

    print(f"\n1ï¸âƒ£ åŸå§‹å½¢çŠ¶æ£€æŸ¥:")
    print(f"   atom_to_text shape: {atom_to_text.shape}")
    batch, num_heads, num_atoms, seq_len = atom_to_text.shape
    print(f"   - Batch size: {batch}")
    print(f"   - Number of heads: {num_heads}")
    print(f"   - Number of atoms: {num_atoms}")
    print(f"   - Sequence length: {seq_len}")

    # Extract first batch
    atom_to_text = atom_to_text[0]  # [heads, num_atoms, seq_len]

    print(f"\n2ï¸âƒ£ æ£€æŸ¥ä¸åŒ Attention Head æ˜¯å¦æœ‰å·®å¼‚:")
    for head in range(num_heads):
        head_data = atom_to_text[head].cpu().numpy()  # [num_atoms, seq_len]
        entropy = -np.sum(head_data * np.log(head_data + 1e-10)) / (num_atoms * seq_len)
        print(f"   Head {head}: Entropy = {entropy:.4f}")

        # Check if all atoms identical in this head
        if num_atoms > 1:
            identical = np.allclose(head_data[0], head_data[1], atol=1e-6)
            if identical:
                print(f"      âš ï¸  Atom 0 å’Œ Atom 1 åœ¨æ­¤ head ä¸­å®Œå…¨ç›¸åŒ!")

    print(f"\n3ï¸âƒ£ å¯¹æ‰€æœ‰ heads å–å¹³å‡åæ£€æŸ¥:")
    atom_to_text_avg = atom_to_text.mean(dim=0).cpu().numpy()  # [num_atoms, seq_len]
    print(f"   å¹³å‡åå½¢çŠ¶: {atom_to_text_avg.shape}")

    # Check each atom's top 5 tokens
    print(f"\n   æ¯ä¸ªåŸå­çš„ Top 5 tokens (å¹³å‡åï¼Œåˆå¹¶å‰):")
    for i in range(min(5, num_atoms)):
        top_5_indices = atom_to_text_avg[i].argsort()[-5:][::-1]
        top_5_weights = atom_to_text_avg[i, top_5_indices]
        element_name = elements[i] if elements else f"Atom_{i}"
        print(f"   {element_name:8s}: indices={top_5_indices}, weights={top_5_weights}")

    # Statistical comparison between atoms
    print(f"\n4ï¸âƒ£ åŸå­é—´ç»Ÿè®¡æ¯”è¾ƒ:")
    if num_atoms > 1:
        # Compare first two atoms
        correlation = np.corrcoef(atom_to_text_avg[0], atom_to_text_avg[1])[0, 1]
        print(f"   Atom 0 å’Œ Atom 1 ç›¸å…³ç³»æ•°: {correlation:.6f}")

        if correlation > 0.99:
            print(f"   âš ï¸  è­¦å‘Šï¼šç›¸å…³ç³»æ•° > 0.99ï¼Œä¸¤ä¸ªåŸå­çš„æ³¨æ„åŠ›æ¨¡å¼å‡ ä¹ç›¸åŒ!")

        # Check if completely identical
        identical = np.allclose(atom_to_text_avg[0], atom_to_text_avg[1], atol=1e-6)
        if identical:
            print(f"   âŒ é”™è¯¯ï¼šAtom 0 å’Œ Atom 1 å®Œå…¨ç›¸åŒ (allclose with atol=1e-6)")
            print(f"      è¿™è¯´æ˜æ¨¡å‹æ²¡æœ‰å­¦åˆ°åŒºåˆ†ä¸åŒåŸå­çš„èƒ½åŠ›!")
        else:
            print(f"   âœ… Atom 0 å’Œ Atom 1 ä¸å®Œå…¨ç›¸åŒ")

        # Check variance across atoms
        atom_means = atom_to_text_avg.mean(axis=1)  # [num_atoms]
        atom_variance = atom_means.var()
        print(f"   åŸå­é—´å¹³å‡æ³¨æ„åŠ›çš„æ–¹å·®: {atom_variance:.6f}")

        if atom_variance < 1e-6:
            print(f"   âš ï¸  è­¦å‘Šï¼šæ–¹å·®æå°ï¼Œæ‰€æœ‰åŸå­å¯èƒ½æœ‰ç›¸åŒçš„å¹³å‡æ³¨æ„åŠ›")

    # Check specific patterns
    print(f"\n5ï¸âƒ£ æ£€æŸ¥æ˜¯å¦æ‰€æœ‰åŸå­å…³æ³¨ç›¸åŒçš„ tokens:")
    top_tokens_per_atom = [atom_to_text_avg[i].argmax() for i in range(min(5, num_atoms))]
    unique_top_tokens = len(set(top_tokens_per_atom))
    print(f"   å‰5ä¸ªåŸå­çš„ top token: {top_tokens_per_atom}")
    print(f"   ç‹¬ç‰¹çš„ top tokens æ•°é‡: {unique_top_tokens} / {min(5, num_atoms)}")

    if unique_top_tokens == 1:
        print(f"   âŒ æ‰€æœ‰åŸå­éƒ½å…³æ³¨åŒä¸€ä¸ª token!")
    elif unique_top_tokens < min(3, num_atoms):
        print(f"   âš ï¸  å¤§éƒ¨åˆ†åŸå­å…³æ³¨ç›¸åŒçš„ tokens")
    else:
        print(f"   âœ… ä¸åŒåŸå­å…³æ³¨ä¸åŒçš„ tokens")

    print(f"\n6ï¸âƒ£ è¯Šæ–­ç»“è®º:")

    # Determine the issue
    issues = []

    # Check if all heads are identical
    all_heads_identical = True
    for head in range(num_heads - 1):
        if not np.allclose(atom_to_text[head].cpu().numpy(),
                          atom_to_text[head + 1].cpu().numpy(), atol=1e-6):
            all_heads_identical = False
            break

    if all_heads_identical:
        issues.append("æ‰€æœ‰ attention heads å®Œå…¨ç›¸åŒ (å¤šå¤´æ³¨æ„åŠ›é€€åŒ–)")

    # Check if all atoms identical within averaged attention
    all_atoms_identical = True
    for i in range(num_atoms - 1):
        if not np.allclose(atom_to_text_avg[i], atom_to_text_avg[i + 1], atol=1e-6):
            all_atoms_identical = False
            break

    if all_atoms_identical:
        issues.append("æ‰€æœ‰åŸå­çš„æ³¨æ„åŠ›æ¨¡å¼å®Œå…¨ç›¸åŒ (fine-grained attention å¤±æ•ˆ)")

    if len(issues) > 0:
        print(f"   âŒ å‘ç°é—®é¢˜:")
        for issue in issues:
            print(f"      - {issue}")
        print(f"\n   ğŸ’¡ å»ºè®®:")
        print(f"      1. æ£€æŸ¥æ¨¡å‹è®­ç»ƒæ˜¯å¦æ­£ç¡® (use_fine_grained_attention=True)")
        print(f"      2. æ£€æŸ¥ checkpoint æ˜¯å¦æ­£ç¡®åŠ è½½")
        print(f"      3. æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨fine-grained attentionä»»åŠ¡ä¸Šè®­ç»ƒè¿‡")
        print(f"      4. å°è¯•å¯è§†åŒ–ä¸åŒçš„æ ·æœ¬ï¼Œçœ‹æ˜¯å¦éƒ½æœ‰æ­¤é—®é¢˜")
    else:
        print(f"   âœ… æœªå‘ç°æ˜æ˜¾é—®é¢˜")
        print(f"      æ¨¡å‹è¾“å‡ºçš„ fine-grained attention çœ‹èµ·æ¥æ­£å¸¸")
        print(f"      å¦‚æœä»ç„¶çœ‹åˆ°ç›¸åŒçš„ top wordsï¼Œé—®é¢˜å¯èƒ½åœ¨åç»­å¤„ç†æ­¥éª¤")

    print("="*80 + "\n")

    return {
        'num_heads': num_heads,
        'num_atoms': num_atoms,
        'all_heads_identical': all_heads_identical,
        'all_atoms_identical': all_atoms_identical,
        'correlation_0_1': correlation if num_atoms > 1 else None,
        'issues': issues
    }


# ä½¿ç”¨ç¤ºä¾‹ (æ·»åŠ åˆ° demo_fine_grained_attention.py ä¸­):
"""
# åœ¨ç¬¬ 166 è¡Œä¹‹åæ·»åŠ :

    # è¯Šæ–­æ¨¡å‹è¾“å‡º
    from diagnose_model_attention import diagnose_fine_grained_attention

    diagnosis = diagnose_fine_grained_attention(
        fg_attn,
        elements=[str(atoms_object.elements[i]) for i in range(atoms_object.num_atoms)]
    )
"""
