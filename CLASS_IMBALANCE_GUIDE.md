# äºŒåˆ†ç±»ä»»åŠ¡ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜æŒ‡å—

## ğŸ¯ å¿«é€Ÿå›ç­”

**æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒä¸å¹³è¡¡æœ‰å½±å“å—ï¼Ÿ** â†’ **æ˜¯çš„ï¼Œæœ‰æ˜¾è‘—å½±å“ï¼**

---

## âš ï¸ ä¸»è¦å½±å“

### 1. è¯„ä¼°æŒ‡æ ‡å¤±çœŸ

å‡è®¾æµ‹è¯•é›†åˆ†å¸ƒï¼š
```
ç±»åˆ«0: 900ä¸ªæ ·æœ¬ (90%)
ç±»åˆ«1: 100ä¸ªæ ·æœ¬ (10%)
```

**æƒ…å†µA**: æ¨¡å‹å…¨éƒ¨é¢„æµ‹ä¸ºç±»åˆ«0
```
å‡†ç¡®ç‡ = 90% âœ…  â† çœ‹èµ·æ¥å¾ˆå¥½ï¼
å¬å›ç‡ï¼ˆç±»åˆ«1ï¼‰= 0% âŒ  â† å®Œå…¨å¤±è´¥ï¼
```

**ç»“è®º**: å‡†ç¡®ç‡ä¼šä¸¥é‡è¯¯å¯¼ï¼Œä¸èƒ½çœŸå®åæ˜ æ¨¡å‹æ€§èƒ½ã€‚

---

### 2. æ¨¡å‹ä¼˜åŒ–åå‘

è®­ç»ƒæ—¶å¦‚æœä¸å¤„ç†ä¸å¹³è¡¡ï¼š

| æ¨¡å‹è¡Œä¸º | åŸå›  | åæœ |
|---------|------|------|
| å€¾å‘é¢„æµ‹å¤šæ•°ç±» | æŸå¤±å‡½æ•°è¢«å¤šæ•°ç±»ä¸»å¯¼ | å°‘æ•°ç±»æ ·æœ¬è¢«å¿½ç•¥ |
| å†³ç­–è¾¹ç•Œåç§» | ä¼˜åŒ–æ•´ä½“å‡†ç¡®ç‡ | å°‘æ•°ç±»åˆ†ç±»é”™è¯¯ç‡é«˜ |
| ç‰¹å¾å­¦ä¹ ä¸å……åˆ† | å°‘æ•°ç±»æ ·æœ¬å°‘ | æ³›åŒ–èƒ½åŠ›å·® |

---

### 3. å¯¹ä¸åŒæŒ‡æ ‡çš„å½±å“

| æŒ‡æ ‡ | å—å½±å“ç¨‹åº¦ | æ˜¯å¦å¯é  | è¯´æ˜ |
|------|----------|---------|------|
| **å‡†ç¡®ç‡** | ğŸ”´ ä¸¥é‡ | âŒ | è¢«å¤šæ•°ç±»ä¸»å¯¼ï¼Œå®¹æ˜“è™šé«˜ |
| **ç²¾ç¡®ç‡** | ğŸŸ¡ ä¸­ç­‰ | âš ï¸ | å¯¹å°‘æ•°ç±»å¯èƒ½ä¸å‡† |
| **å¬å›ç‡** | ğŸ”´ ä¸¥é‡ | âŒ | å°‘æ•°ç±»å¬å›ç‡é€šå¸¸å¾ˆä½ |
| **F1åˆ†æ•°** | ğŸŸ¢ è½»å¾® | âœ… | å¹³è¡¡äº†ç²¾ç¡®ç‡å’Œå¬å›ç‡ |
| **ROC-AUC** | ğŸŸ¢ è½»å¾® | âœ… | å¯¹ä¸å¹³è¡¡ç›¸å¯¹é²æ£’ |
| **PR-AUC** | ğŸŸ¢ è½»å¾® | âœ… | æ›´é€‚åˆä¸å¹³è¡¡æ•°æ® |

---

## ğŸ” æ£€æŸ¥ä½ çš„æ•°æ®åˆ†å¸ƒ

### æ–¹æ³•1: ä½¿ç”¨æ£€æŸ¥è„šæœ¬

```bash
python check_class_distribution.py /path/to/your/id_prop.csv
```

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸ“Š æ•°æ®é›†æ€»æ ·æœ¬æ•°: 1000

ç±»åˆ«åˆ†å¸ƒ:
------------------------------------------------------------
  ç±»åˆ« 0:    900 æ ·æœ¬ (90.00%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  ç±»åˆ« 1:    100 æ ·æœ¬ (10.00%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

ä¸å¹³è¡¡åˆ†æ:
------------------------------------------------------------
  å¤šæ•°ç±» (ç±»åˆ«0): 900 æ ·æœ¬
  å°‘æ•°ç±» (ç±»åˆ«1): 100 æ ·æœ¬
  ä¸å¹³è¡¡æ¯”ç‡: 9.00:1

ä¸¥é‡ç¨‹åº¦: ğŸŸ¡ ä¸­åº¦ä¸å¹³è¡¡
å»ºè®®: å»ºè®®ä½¿ç”¨ç±»åˆ«æƒé‡æˆ–è°ƒæ•´æŸå¤±å‡½æ•°

ğŸ’¡ æ¨èé…ç½®:
------------------------------------------------------------
  pos_weight (ç”¨äºBCEWithLogitsLoss): 9.0000
  class_weight={0: 1.0, 1: 9.0000}
```

### æ–¹æ³•2: å¿«é€Ÿç»Ÿè®¡

```python
import pandas as pd

df = pd.read_csv('your_data.csv')
print(df['target'].value_counts())
print(df['target'].value_counts(normalize=True))
```

---

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### ç­–ç•¥1: ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•° â­ æ¨è

**åŸç†**: ç»™å°‘æ•°ç±»æ›´é«˜çš„æƒé‡ï¼Œè®©æ¨¡å‹æ›´å…³æ³¨å°‘æ•°ç±»ã€‚

#### a) BCEWithLogitsLoss + pos_weight

```python
# è®¡ç®—pos_weight
num_class_0 = 900
num_class_1 = 100
pos_weight = num_class_0 / num_class_1  # 9.0

# åˆ›å»ºæŸå¤±å‡½æ•°
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))
```

**ä¼˜ç‚¹**:
- âœ… ç®€å•æœ‰æ•ˆ
- âœ… ä¸æ”¹å˜æ•°æ®åˆ†å¸ƒ
- âœ… è®¡ç®—å¼€é”€å°

**è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨**:
```bash
python train_with_cross_modal_attention.py \
    --classification 1 \
    --pos_weight 9.0 \
    ...
```

#### b) è‡ªå®šä¹‰åŠ æƒBCEæŸå¤±

```python
class WeightedBCELoss(nn.Module):
    def __init__(self, weight_pos=1.0):
        super().__init__()
        self.weight_pos = weight_pos

    def forward(self, pred, target):
        loss = -(self.weight_pos * target * torch.log(pred + 1e-8) +
                 (1 - target) * torch.log(1 - pred + 1e-8))
        return loss.mean()

# ä½¿ç”¨
criterion = WeightedBCELoss(weight_pos=9.0)
```

---

### ç­–ç•¥2: æ•°æ®é‡é‡‡æ ·

#### a) è¿‡é‡‡æ ·ï¼ˆOversamplingï¼‰

**åŸç†**: å¤åˆ¶å°‘æ•°ç±»æ ·æœ¬

```python
from imblearn.over_sampling import RandomOverSampler, SMOTE

# æ–¹æ³•1: éšæœºè¿‡é‡‡æ ·
ros = RandomOverSampler(random_state=42)
X_resampled, y_resampled = ros.fit_resample(X, y)

# æ–¹æ³•2: SMOTEï¼ˆç”Ÿæˆåˆæˆæ ·æœ¬ï¼‰
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
```

**ä¼˜ç‚¹**:
- âœ… å¢åŠ å°‘æ•°ç±»æ ·æœ¬æ•°é‡
- âœ… SMOTEå¯ä»¥ç”Ÿæˆå¤šæ ·æ€§

**ç¼ºç‚¹**:
- âš ï¸ å¯èƒ½è¿‡æ‹Ÿåˆå°‘æ•°ç±»
- âš ï¸ å¢åŠ è®­ç»ƒæ—¶é—´

#### b) æ¬ é‡‡æ ·ï¼ˆUndersamplingï¼‰

**åŸç†**: å‡å°‘å¤šæ•°ç±»æ ·æœ¬

```python
from imblearn.under_sampling import RandomUnderSampler

rus = RandomUnderSampler(random_state=42)
X_resampled, y_resampled = rus.fit_resample(X, y)
```

**ä¼˜ç‚¹**:
- âœ… å¹³è¡¡æ•°æ®é›†
- âœ… å‡å°‘è®­ç»ƒæ—¶é—´

**ç¼ºç‚¹**:
- âš ï¸ ä¸¢å¤±å¤šæ•°ç±»ä¿¡æ¯

---

### ç­–ç•¥3: è°ƒæ•´å†³ç­–é˜ˆå€¼

**åŸç†**: ä¸ä½¿ç”¨é»˜è®¤çš„0.5é˜ˆå€¼ï¼Œæ ¹æ®éªŒè¯é›†è°ƒæ•´ã€‚

```python
from sklearn.metrics import precision_recall_curve

# åœ¨éªŒè¯é›†ä¸Šæ‰¾æœ€ä½³é˜ˆå€¼
precisions, recalls, thresholds = precision_recall_curve(y_val, pred_probs)
f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
best_threshold = thresholds[np.argmax(f1_scores)]

print(f"æœ€ä½³é˜ˆå€¼: {best_threshold:.4f}")

# ä½¿ç”¨æœ€ä½³é˜ˆå€¼é¢„æµ‹
y_pred = (pred_probs >= best_threshold).astype(int)
```

---

### ç­–ç•¥4: é›†æˆæ–¹æ³•

**åŸç†**: è®­ç»ƒå¤šä¸ªæ¨¡å‹å¤„ç†ä¸åŒçš„æ•°æ®å­é›†ã€‚

```python
# BalancedBaggingClassifier
from imblearn.ensemble import BalancedBaggingClassifier

model = BalancedBaggingClassifier(
    base_estimator=your_model,
    n_estimators=10,
    random_state=42
)
```

---

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡é€‰æ‹©

### âŒ ä¸æ¨èçš„æŒ‡æ ‡

| æŒ‡æ ‡ | é—®é¢˜ |
|------|------|
| **å‡†ç¡®ç‡** | è¢«å¤šæ•°ç±»ä¸»å¯¼ï¼Œå®¹æ˜“è¯¯å¯¼ |

### âœ… æ¨èçš„æŒ‡æ ‡

#### 1. F1åˆ†æ•°ï¼ˆæœ€é‡è¦ï¼‰

```python
from sklearn.metrics import f1_score

# Macro F1: æ¯ä¸ªç±»åˆ«F1çš„å¹³å‡ï¼ˆç»™å°‘æ•°ç±»æ›´å¤šæƒé‡ï¼‰
f1_macro = f1_score(y_true, y_pred, average='macro')

# Weighted F1: æŒ‰æ ·æœ¬æ•°åŠ æƒ
f1_weighted = f1_score(y_true, y_pred, average='weighted')
```

**æ¨èä½¿ç”¨ Macro F1**ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®æ›´æ•æ„Ÿã€‚

#### 2. ç²¾ç¡®ç‡å’Œå¬å›ç‡

```python
from sklearn.metrics import precision_score, recall_score

precision = precision_score(y_true, y_pred)  # æ­£ç¡®é¢„æµ‹çš„æ­£æ ·æœ¬æ¯”ä¾‹
recall = recall_score(y_true, y_pred)        # æ­£æ ·æœ¬è¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹
```

**å°‘æ•°ç±»çš„å¬å›ç‡**ç‰¹åˆ«é‡è¦ï¼

#### 3. ROC-AUC

```python
from sklearn.metrics import roc_auc_score

roc_auc = roc_auc_score(y_true, pred_probs)
```

**ä¼˜ç‚¹**: å¯¹ä¸å¹³è¡¡ç›¸å¯¹é²æ£’

#### 4. PR-AUCï¼ˆæ¨èï¼‰

```python
from sklearn.metrics import average_precision_score

pr_auc = average_precision_score(y_true, pred_probs)
```

**ä¼˜ç‚¹**: æ¯”ROC-AUCæ›´é€‚åˆä¸å¹³è¡¡æ•°æ®

---

## ğŸš€ å®æˆ˜ï¼šä¿®æ”¹ä½ çš„è®­ç»ƒè„šæœ¬

### æ­¥éª¤1: æ£€æŸ¥æ•°æ®åˆ†å¸ƒ

```bash
python check_class_distribution.py /public/home/ghzhang/crysmmnet-main/dataset/jarvis/mbj_bandgap/id_prop.csv
```

**è®°ä¸‹è¾“å‡ºçš„ `pos_weight` å€¼ï¼**

### æ­¥éª¤2: ä½¿ç”¨åŠ æƒæŸå¤±è®­ç»ƒ

å‡è®¾ `pos_weight = 9.0`ï¼š

```bash
export HF_ENDPOINT=https://hf-mirror.com
export CUDA_VISIBLE_DEVICES=0

python train_with_cross_modal_attention.py \
    --root_dir /public/home/ghzhang/crysmmnet-main/dataset \
    --dataset jarvis \
    --property mbj_bandgap \
    --batch_size 128 \
    --epochs 100 \
    --classification 1 \
    --pos_weight 9.0 \
    --use_fine_grained_attention True \
    --use_only_graph_for_prediction True \
    --output_dir ./output_classification_balanced \
    --random_seed 42
```

### æ­¥éª¤3: ä½¿ç”¨æ­£ç¡®çš„è¯„ä¼°æŒ‡æ ‡

è®­ç»ƒå®Œæˆåï¼Œè¯„ä¼°æ—¶å…³æ³¨ï¼š

```python
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score

# è¯¦ç»†æŠ¥å‘Šï¼ˆåŒ…å«æ¯ä¸ªç±»åˆ«çš„precision/recall/f1ï¼‰
print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1']))

# ROC-AUC
print(f"ROC-AUC: {roc_auc_score(y_test, pred_probs):.4f}")

# PR-AUCï¼ˆæ¨èï¼‰
print(f"PR-AUC: {average_precision_score(y_test, pred_probs):.4f}")
```

---

## ğŸ“ˆ ä¸å¹³è¡¡ä¸¥é‡ç¨‹åº¦è¯„ä¼°

| ä¸å¹³è¡¡æ¯”ç‡ | ä¸¥é‡ç¨‹åº¦ | å¿…é¡»é‡‡å–çš„æªæ–½ |
|-----------|---------|--------------|
| < 3:1 | ğŸŸ¢ è½»åº¦ | å¯é€‰ï¼šç±»åˆ«æƒé‡ |
| 3:1 - 10:1 | ğŸŸ¡ ä¸­åº¦ | **å¿…é¡»**ï¼šç±»åˆ«æƒé‡æˆ–é‡é‡‡æ · |
| > 10:1 | ğŸ”´ ä¸¥é‡ | **å¿…é¡»**ï¼šç±»åˆ«æƒé‡ + é‡é‡‡æ · + è°ƒæ•´é˜ˆå€¼ |

---

## ğŸ’¡ æœ€ä½³å®è·µæ€»ç»“

### è®­ç»ƒé›†ä¸å¹³è¡¡

1. âœ… **ä½¿ç”¨åŠ æƒæŸå¤±å‡½æ•°**ï¼ˆpos_weightï¼‰
2. âœ… è€ƒè™‘è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰
3. âœ… ä½¿ç”¨Focal Lossï¼ˆå¯¹éš¾åˆ†æ ·æœ¬åŠ æƒï¼‰
4. âš ï¸ é¿å…æ¬ é‡‡æ ·ï¼ˆé™¤éæ•°æ®é‡å¾ˆå¤§ï¼‰

### éªŒè¯é›†/æµ‹è¯•é›†ä¸å¹³è¡¡

1. âœ… **ä½¿ç”¨F1åˆ†æ•°ã€ROC-AUCã€PR-AUCè¯„ä¼°**
2. âœ… åˆ†åˆ«æŠ¥å‘Šæ¯ä¸ªç±»åˆ«çš„ç²¾ç¡®ç‡å’Œå¬å›ç‡
3. âœ… åœ¨éªŒè¯é›†ä¸Šè°ƒæ•´å†³ç­–é˜ˆå€¼
4. âŒ **ä¸è¦åªçœ‹å‡†ç¡®ç‡**

### ç»¼åˆç­–ç•¥

```
è®­ç»ƒæ—¶: åŠ æƒæŸå¤± + è¿‡é‡‡æ ·ï¼ˆSMOTEï¼‰
è¯„ä¼°æ—¶: F1-macro + PR-AUC + æ··æ·†çŸ©é˜µ
è°ƒä¼˜æ—¶: ç½‘æ ¼æœç´¢æœ€ä½³é˜ˆå€¼
```

---

## ğŸ”§ ä»£ç ç¤ºä¾‹

### å®Œæ•´çš„ä¸å¹³è¡¡åˆ†ç±»æµç¨‹

```python
import torch
import torch.nn as nn
from sklearn.metrics import classification_report, roc_auc_score, average_precision_score
import numpy as np

# 1. è®¡ç®—ç±»åˆ«æƒé‡
def compute_pos_weight(y_train):
    """è®¡ç®—BCEWithLogitsLossçš„pos_weight"""
    num_pos = (y_train == 1).sum()
    num_neg = (y_train == 0).sum()
    pos_weight = num_neg / num_pos
    return pos_weight

# 2. åˆ›å»ºåŠ æƒæŸå¤±
pos_weight = compute_pos_weight(y_train)
print(f"Pos weight: {pos_weight:.4f}")
criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight]))

# 3. è®­ç»ƒï¼ˆç¤ºä¾‹ï¼‰
model.train()
for epoch in range(num_epochs):
    for batch in train_loader:
        logits = model(batch)
        loss = criterion(logits, batch_labels)
        # ... backward and optimize

# 4. è¯„ä¼°
model.eval()
all_preds = []
all_probs = []
all_labels = []

with torch.no_grad():
    for batch in test_loader:
        logits = model(batch)
        probs = torch.sigmoid(logits)
        preds = (probs > 0.5).int()

        all_probs.extend(probs.cpu().numpy())
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(batch_labels.cpu().numpy())

all_preds = np.array(all_preds)
all_probs = np.array(all_probs)
all_labels = np.array(all_labels)

# 5. æŠ¥å‘Šç»“æœ
print("\nåˆ†ç±»æŠ¥å‘Š:")
print(classification_report(all_labels, all_preds,
                          target_names=['Class 0', 'Class 1']))

print(f"\nROC-AUC: {roc_auc_score(all_labels, all_probs):.4f}")
print(f"PR-AUC: {average_precision_score(all_labels, all_probs):.4f}")

# 6. æ··æ·†çŸ©é˜µ
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(all_labels, all_preds)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

### å­¦æœ¯è®ºæ–‡
- **Focal Loss**: "Focal Loss for Dense Object Detection" (Lin et al., 2017)
- **SMOTE**: "SMOTE: Synthetic Minority Over-sampling Technique" (Chawla et al., 2002)

### å·¥å…·åº“
- **imbalanced-learn**: https://imbalanced-learn.org/
- **PyTorch åŠ æƒæŸå¤±**: https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss

---

## ğŸ¯ å¿«é€Ÿå†³ç­–æ ‘

```
æµ‹è¯•é›†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ
    â†“
    â”œâ”€ æ˜¯ â†’ è®¡ç®—ä¸å¹³è¡¡æ¯”ç‡
    â”‚       â†“
    â”‚       â”œâ”€ < 3:1 â†’ å¯ä»¥åªç”¨F1åˆ†æ•°è¯„ä¼°
    â”‚       â”œâ”€ 3:1 - 10:1 â†’ ä½¿ç”¨pos_weight + F1/PR-AUCè¯„ä¼°
    â”‚       â””â”€ > 10:1 â†’ pos_weight + é‡é‡‡æ · + é˜ˆå€¼è°ƒæ•´ + PR-AUC
    â”‚
    â””â”€ å¦ â†’ ä½¿ç”¨æ ‡å‡†æµç¨‹ï¼ˆå‡†ç¡®ç‡å¯ä¿¡ï¼‰
```

---

## âš™ï¸ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **æ£€æŸ¥æ•°æ®åˆ†å¸ƒ**:
   ```bash
   python check_class_distribution.py your_data.csv
   ```

2. **é€‰æ‹©ç­–ç•¥** (æ ¹æ®ä¸å¹³è¡¡æ¯”ç‡)

3. **ä¿®æ”¹è®­ç»ƒè„šæœ¬** (æ·»åŠ pos_weight)

4. **ä½¿ç”¨æ­£ç¡®çš„è¯„ä¼°æŒ‡æ ‡** (F1-macro, PR-AUC)

5. **å¯¹æ¯”å®éªŒ** (æœ‰æƒé‡ vs æ— æƒé‡)

---

éœ€è¦æˆ‘å¸®ä½ ä¿®æ”¹è®­ç»ƒè„šæœ¬æ¥æ”¯æŒç±»åˆ«æƒé‡å—ï¼Ÿ
