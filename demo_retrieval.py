#!/usr/bin/env python
"""
ç®€åŒ–çš„ Retrieval è¯„ä¼°ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ RetrievalEvaluator
"""

import torch
from evaluate_retrieval import RetrievalEvaluator
from models.alignn import ALIGNN, ALIGNNConfig
from data import get_train_val_loaders
from utils_retrieval import load_model_checkpoint


def simple_retrieval_demo():
    """ç®€å•çš„æ£€ç´¢è¯„ä¼°ç¤ºä¾‹"""

    # ========== 1. åˆå§‹åŒ–æ¨¡å‹ ==========
    print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    model_config = ALIGNNConfig(
        name="alignn",
        classification=True,
        use_cross_modal_attention=True,    # ä½¿ç”¨è·¨æ¨¡æ€æ³¨æ„åŠ›
        use_fine_grained_attention=False,  # å¯é€‰ï¼šç»†ç²’åº¦æ³¨æ„åŠ›
        use_middle_fusion=True,            # ä½¿ç”¨ä¸­æœŸèåˆï¼ˆæé«˜å¯¹é½ï¼‰
        middle_fusion_layers="2",          # åœ¨ç¬¬2å±‚èåˆ
        graph_dropout=0.0                  # è¯„ä¼°æ—¶ä¸ç”¨ dropout
    )
    model = ALIGNN(model_config)

    # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
    checkpoint_path = "checkpoints/best_model.pt"

    # ä½¿ç”¨æ™ºèƒ½åŠ è½½å‡½æ•°ï¼ˆè‡ªåŠ¨å¤„ç†ä¸åŒçš„æ£€æŸ¥ç‚¹æ ¼å¼ï¼‰
    model, checkpoint_info = load_model_checkpoint(
        model, checkpoint_path, device='cuda', verbose=True
    )

    # ========== 2. åŠ è½½æ•°æ® ==========
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    train_loader, val_loader, test_loader = get_train_val_loaders(
        dataset="your_dataset_path",
        target="target_property",
        batch_size=32,
        workers=4
    )

    # ========== 3. åˆ›å»ºè¯„ä¼°å™¨ ==========
    evaluator = RetrievalEvaluator(model, device='cuda')

    # ========== 4. è¿è¡Œè¯„ä¼° ==========
    print("\n" + "="*80)
    print("ğŸ¯ å¼€å§‹è¯„ä¼°æ£€ç´¢æ€§èƒ½...")
    print("="*80 + "\n")

    metrics = evaluator.evaluate(
        dataloader=val_loader,
        max_samples=1000,           # è¯„ä¼° 1000 ä¸ªæ ·æœ¬ï¼ˆæ›´å¿«ï¼‰
        k_values=[1, 5, 10, 20],   # è®¡ç®— R@1, R@5, R@10, R@20
        visualize=True,             # ç”Ÿæˆå¯è§†åŒ–
        output_dir='./retrieval_results'
    )

    # ========== 5. è§£è¯»ç»“æœ ==========
    print("\n" + "="*80)
    print("ğŸ“Š ç»“æœè§£è¯»:")
    print("="*80)

    g2t_r1 = metrics['g2t_R@1'] * 100
    t2g_r1 = metrics['t2g_R@1'] * 100
    avg_r1 = metrics['avg_R@1'] * 100

    print(f"\nâœ¨ R@1 æ€§èƒ½:")
    print(f"   - Graphâ†’Text: {g2t_r1:.2f}%")
    print(f"   - Textâ†’Graph: {t2g_r1:.2f}%")
    print(f"   - å¹³å‡: {avg_r1:.2f}%")

    # æ€§èƒ½è¯„çº§
    if avg_r1 >= 80:
        grade = "ğŸ† ä¼˜ç§€ï¼æ¨¡å‹çš„å›¾-æ–‡æœ¬å¯¹é½èƒ½åŠ›éå¸¸å¼º"
    elif avg_r1 >= 60:
        grade = "ğŸ‘ è‰¯å¥½ï¼ä¸­æœŸèåˆèµ·ä½œç”¨äº†"
    elif avg_r1 >= 40:
        grade = "ğŸ˜ ä¸€èˆ¬ï¼Œè¿˜æœ‰æå‡ç©ºé—´"
    else:
        grade = "âŒ è¾ƒå·®ï¼Œå»ºè®®æ£€æŸ¥èåˆç­–ç•¥"

    print(f"\nè¯„çº§: {grade}")

    print(f"\nğŸ’¡ å»ºè®®:")
    if avg_r1 < 60:
        print("   - æ£€æŸ¥ use_middle_fusion æ˜¯å¦å¼€å¯")
        print("   - å°è¯•å¢åŠ  contrastive_loss_weight")
        print("   - è€ƒè™‘ä½¿ç”¨ use_fine_grained_attention")
    else:
        print("   - æ¨¡å‹å¯¹é½èƒ½åŠ›å·²ç»å¾ˆå¥½ï¼")
        print("   - å¯ä»¥è€ƒè™‘å¢åŠ  graph_dropout è¿›è¡Œæ­£åˆ™åŒ–")

    return metrics


def quick_retrieval_check(model, dataloader, num_samples=100):
    """
    å¿«é€Ÿæ£€æŸ¥æ£€ç´¢æ€§èƒ½ï¼ˆä¸ä¿å­˜ç»“æœï¼‰

    ç”¨äºè®­ç»ƒè¿‡ç¨‹ä¸­å¿«é€Ÿè¯„ä¼°
    """
    evaluator = RetrievalEvaluator(model, device='cuda')

    # æå–ç‰¹å¾
    graph_features, text_features, _ = evaluator.extract_features(
        dataloader, max_samples=num_samples
    )

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarity_matrix = evaluator.compute_similarity_matrix(
        graph_features, text_features
    )

    # åªè®¡ç®— R@1
    N = similarity_matrix.size(0)
    correct_indices = torch.arange(N)

    # Graph-to-Text R@1
    _, sorted_indices = torch.sort(similarity_matrix, dim=1, descending=True)
    top_1_indices = sorted_indices[:, 0]
    g2t_r1 = (top_1_indices == correct_indices).float().mean().item()

    # Text-to-Graph R@1
    _, sorted_indices = torch.sort(similarity_matrix, dim=0, descending=True)
    top_1_indices = sorted_indices[0, :]
    t2g_r1 = (top_1_indices == correct_indices).float().mean().item()

    avg_r1 = (g2t_r1 + t2g_r1) / 2

    return {
        'g2t_R@1': g2t_r1,
        't2g_R@1': t2g_r1,
        'avg_R@1': avg_r1
    }


def compare_models_retrieval(model_paths, dataloader, labels):
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ£€ç´¢æ€§èƒ½

    Args:
        model_paths: æ¨¡å‹è·¯å¾„åˆ—è¡¨
        dataloader: æ•°æ®åŠ è½½å™¨
        labels: æ¨¡å‹æ ‡ç­¾åˆ—è¡¨
    """
    import matplotlib.pyplot as plt

    results = []

    for model_path, label in zip(model_paths, labels):
        print(f"\nè¯„ä¼°æ¨¡å‹: {label}")
        print("-" * 60)

        # åŠ è½½æ¨¡å‹
        model_config = ALIGNNConfig(name="alignn", classification=True)
        model = ALIGNN(model_config)

        # ä½¿ç”¨æ™ºèƒ½åŠ è½½å‡½æ•°
        model, _ = load_model_checkpoint(model, model_path, device='cuda', verbose=False)

        # å¿«é€Ÿè¯„ä¼°
        metrics = quick_retrieval_check(model, dataloader, num_samples=500)

        results.append({
            'label': label,
            'metrics': metrics
        })

        print(f"R@1 = {metrics['avg_R@1']*100:.2f}%")

    # å¯è§†åŒ–æ¯”è¾ƒ
    labels_list = [r['label'] for r in results]
    g2t_values = [r['metrics']['g2t_R@1'] * 100 for r in results]
    t2g_values = [r['metrics']['t2g_R@1'] * 100 for r in results]
    avg_values = [r['metrics']['avg_R@1'] * 100 for r in results]

    x = range(len(labels_list))
    width = 0.25

    fig, ax = plt.subplots(figsize=(12, 6))
    ax.bar([i - width for i in x], g2t_values, width, label='Graphâ†’Text', alpha=0.8)
    ax.bar(x, t2g_values, width, label='Textâ†’Graph', alpha=0.8)
    ax.bar([i + width for i in x], avg_values, width, label='Average', alpha=0.8)

    ax.set_xlabel('Models', fontsize=12)
    ax.set_ylabel('R@1 (%)', fontsize=12)
    ax.set_title('æ¨¡å‹æ£€ç´¢æ€§èƒ½å¯¹æ¯”', fontsize=14)
    ax.set_xticks(x)
    ax.set_xticklabels(labels_list, rotation=45, ha='right')
    ax.legend()
    ax.grid(axis='y', alpha=0.3)

    plt.tight_layout()
    plt.savefig('model_comparison_retrieval.png', dpi=300)
    print("\nğŸ’¾ å¯¹æ¯”å›¾å·²ä¿å­˜: model_comparison_retrieval.png")

    return results


if __name__ == '__main__':
    # è¿è¡Œç®€å•ç¤ºä¾‹
    simple_retrieval_demo()

    # æˆ–è€…æ¯”è¾ƒä¸åŒæ¨¡å‹
    # compare_models_retrieval(
    #     model_paths=[
    #         'checkpoints/no_fusion.pt',
    #         'checkpoints/middle_fusion.pt',
    #         'checkpoints/fine_grained.pt'
    #     ],
    #     dataloader=val_loader,
    #     labels=['No Fusion', 'Middle Fusion', 'Fine-Grained']
    # )
