#!/usr/bin/env python3
"""
å…¨æ–°çš„ç»†ç²’åº¦æ³¨æ„åŠ›åˆ†æç³»ç»Ÿ
==========================

ç‰¹æ€§ï¼š
1. è‡ªåŠ¨è¯Šæ–­æ³¨æ„åŠ›æ¨¡å¼æ˜¯å¦æ­£å¸¸
2. å…¼å®¹ä¸åŒç‰ˆæœ¬çš„æ¨¡å‹ä»£ç 
3. å³ä½¿æ‰€æœ‰åŸå­æ³¨æ„åŠ›ç›¸åŒä¹Ÿèƒ½æä¾›æœ‰ç”¨åˆ†æ
4. æä¾›å¤šç§å¯è§†åŒ–å’Œç»Ÿè®¡åˆ†æ
5. è‡ªåŠ¨æ£€æµ‹å’ŒæŠ¥å‘Šå¼‚å¸¸

ä½œè€…: Enhanced Analysis System
æ—¥æœŸ: 2025-11
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import warnings


class RobustAttentionAnalyzer:
    """å¥å£®çš„æ³¨æ„åŠ›åˆ†æå™¨ - èƒ½å¤„ç†å„ç§è¾¹ç•Œæƒ…å†µ"""

    def __init__(self, model=None, device='cuda'):
        """
        Args:
            model: å¯é€‰çš„æ¨¡å‹ï¼ˆç”¨äºç‰¹å¾æå–ï¼‰
            device: è®¡ç®—è®¾å¤‡
        """
        self.model = model
        self.device = device

        # åœç”¨è¯åˆ—è¡¨
        self.stopwords = {
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
            'of', 'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been',
            'be', 'have', 'has', 'had', 'do', 'does', 'did', '[cls]', '[sep]', '[pad]',
            '##s', '##ed', '##ing', '##ly'
        }

    def diagnose_attention_quality(self,
                                   attention_weights: Dict[str, torch.Tensor],
                                   elements: List[str],
                                   verbose: bool = True) -> Dict:
        """
        è¯Šæ–­æ³¨æ„åŠ›æƒé‡è´¨é‡ï¼Œè‡ªåŠ¨æ£€æµ‹é—®é¢˜

        Args:
            attention_weights: åŒ…å« 'atom_to_text' å’Œ 'text_to_atom' çš„å­—å…¸
            elements: åŸå­å…ƒç´ åˆ—è¡¨
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            è¯Šæ–­ç»“æœå­—å…¸
        """

        if verbose:
            print("\n" + "="*80)
            print("ğŸ”¬ æ³¨æ„åŠ›æƒé‡è´¨é‡è¯Šæ–­")
            print("="*80)

        diagnosis = {
            'quality': 'unknown',
            'issues': [],
            'recommendations': [],
            'atom_diversity': 0.0,
            'head_diversity': 0.0,
            'entropy': 0.0,
            'use_alternative_analysis': False
        }

        # æå– atom_to_text æ³¨æ„åŠ›
        atom_to_text = attention_weights.get('atom_to_text', None)
        if atom_to_text is None:
            diagnosis['quality'] = 'missing'
            diagnosis['issues'].append("ç¼ºå°‘ atom_to_text æ³¨æ„åŠ›æƒé‡")
            return diagnosis

        # è½¬æ¢ä¸º numpy
        if isinstance(atom_to_text, torch.Tensor):
            atom_to_text = atom_to_text.cpu().numpy()

        # [batch, heads, num_atoms, seq_len]
        if len(atom_to_text.shape) == 4:
            atom_to_text = atom_to_text[0]  # å–ç¬¬ä¸€ä¸ªbatch

        num_heads, num_atoms, seq_len = atom_to_text.shape

        if verbose:
            print(f"\n1ï¸âƒ£ åŸºæœ¬ä¿¡æ¯:")
            print(f"   - Attention heads: {num_heads}")
            print(f"   - Atoms: {num_atoms}")
            print(f"   - Sequence length: {seq_len}")

        # æ£€æŸ¥1: ä¸åŒheadæ˜¯å¦æœ‰å·®å¼‚
        head_correlations = []
        for i in range(num_heads - 1):
            corr = np.corrcoef(
                atom_to_text[i].flatten(),
                atom_to_text[i + 1].flatten()
            )[0, 1]
            head_correlations.append(corr)

        avg_head_corr = np.mean(head_correlations)
        diagnosis['head_diversity'] = 1.0 - avg_head_corr

        if verbose:
            print(f"\n2ï¸âƒ£ å¤šå¤´æ³¨æ„åŠ›åˆ†æ:")
            print(f"   - å¹³å‡å¤´é—´ç›¸å…³æ€§: {avg_head_corr:.4f}")
            print(f"   - å¤´å¤šæ ·æ€§åˆ†æ•°: {diagnosis['head_diversity']:.4f}")

        if avg_head_corr > 0.99:
            diagnosis['issues'].append("æ‰€æœ‰attention headså‡ ä¹ç›¸åŒï¼ˆå¤šå¤´é€€åŒ–ï¼‰")

        # æ£€æŸ¥2: ä¸åŒåŸå­æ˜¯å¦æœ‰å·®å¼‚
        atom_to_text_avg = atom_to_text.mean(axis=0)  # [num_atoms, seq_len]

        atom_correlations = []
        for i in range(num_atoms - 1):
            if num_atoms > 1:
                corr = np.corrcoef(
                    atom_to_text_avg[i],
                    atom_to_text_avg[i + 1]
                )[0, 1]
                atom_correlations.append(corr)

        if atom_correlations:
            avg_atom_corr = np.mean(atom_correlations)
            diagnosis['atom_diversity'] = 1.0 - avg_atom_corr

            if verbose:
                print(f"\n3ï¸âƒ£ åŸå­ç‰¹å¼‚æ€§åˆ†æ:")
                print(f"   - å¹³å‡åŸå­é—´ç›¸å…³æ€§: {avg_atom_corr:.4f}")
                print(f"   - åŸå­å¤šæ ·æ€§åˆ†æ•°: {diagnosis['atom_diversity']:.4f}")

            if avg_atom_corr > 0.99:
                diagnosis['issues'].append("æ‰€æœ‰åŸå­çš„æ³¨æ„åŠ›æ¨¡å¼å‡ ä¹ç›¸åŒ")
                diagnosis['use_alternative_analysis'] = True

        # æ£€æŸ¥3: æ³¨æ„åŠ›åˆ†å¸ƒçš„ç†µï¼ˆæ˜¯å¦è¿‡äºé›†ä¸­ï¼‰
        # è®¡ç®—æ¯ä¸ªåŸå­çš„æ³¨æ„åŠ›ç†µ
        entropies = []
        for i in range(num_atoms):
            p = atom_to_text_avg[i] + 1e-10  # é¿å…log(0)
            entropy = -np.sum(p * np.log(p))
            entropies.append(entropy)

        diagnosis['entropy'] = np.mean(entropies)

        if verbose:
            print(f"\n4ï¸âƒ£ æ³¨æ„åŠ›åˆ†å¸ƒåˆ†æ:")
            print(f"   - å¹³å‡ç†µ: {diagnosis['entropy']:.4f}")
            print(f"   - æœ€å¤§å¯èƒ½ç†µ: {np.log(seq_len):.4f}")

        if diagnosis['entropy'] < 1.0:
            diagnosis['issues'].append("æ³¨æ„åŠ›åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼ˆä½ç†µï¼‰")

        # ç»¼åˆè¯„ä¼°
        if len(diagnosis['issues']) == 0:
            diagnosis['quality'] = 'good'
        elif len(diagnosis['issues']) <= 2:
            diagnosis['quality'] = 'acceptable'
        else:
            diagnosis['quality'] = 'poor'

        # ç”Ÿæˆå»ºè®®
        if diagnosis['use_alternative_analysis']:
            diagnosis['recommendations'].append(
                "å»ºè®®ä½¿ç”¨å…¨å±€åˆ†æè€Œéé€åŸå­åˆ†æ"
            )
            diagnosis['recommendations'].append(
                "æ£€æŸ¥GNNå±‚è¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦è¿‡äºç›¸ä¼¼"
            )
            diagnosis['recommendations'].append(
                "è€ƒè™‘å‡å°‘GNNå±‚æ•°æˆ–æ·»åŠ æ®‹å·®è¿æ¥"
            )

        if diagnosis['head_diversity'] < 0.1:
            diagnosis['recommendations'].append(
                "å¤šå¤´æ³¨æ„åŠ›é€€åŒ–ï¼Œè€ƒè™‘å¢åŠ head diversityæ­£åˆ™åŒ–"
            )

        if verbose:
            print(f"\n5ï¸âƒ£ è¯Šæ–­ç»“è®º:")
            print(f"   - è´¨é‡è¯„ä¼°: {diagnosis['quality'].upper()}")
            if diagnosis['issues']:
                print(f"   - å‘ç°é—®é¢˜:")
                for issue in diagnosis['issues']:
                    print(f"      â€¢ {issue}")
            if diagnosis['recommendations']:
                print(f"   - å»ºè®®:")
                for rec in diagnosis['recommendations']:
                    print(f"      â€¢ {rec}")
            print("="*80 + "\n")

        return diagnosis

    def analyze_with_fallback(self,
                              attention_weights: Dict[str, torch.Tensor],
                              atoms_object,
                              text_tokens: List[str],
                              save_dir: Optional[Path] = None,
                              top_k: int = 15) -> Dict:
        """
        å¸¦é™çº§ç­–ç•¥çš„åˆ†æï¼šå¦‚æœé€åŸå­åˆ†æå¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å…¨å±€åˆ†æ

        Args:
            attention_weights: æ³¨æ„åŠ›æƒé‡å­—å…¸
            atoms_object: JARVIS Atomså¯¹è±¡
            text_tokens: æ–‡æœ¬tokenåˆ—è¡¨
            save_dir: ä¿å­˜ç›®å½•
            top_k: æ˜¾ç¤ºtop-kç»“æœ

        Returns:
            åˆ†æç»“æœå­—å…¸
        """
        elements = [str(atoms_object.elements[i]) for i in range(atoms_object.num_atoms)]

        # é¦–å…ˆè¯Šæ–­è´¨é‡
        diagnosis = self.diagnose_attention_quality(
            attention_weights, elements, verbose=True
        )

        results = {'diagnosis': diagnosis}

        # æ ¹æ®è¯Šæ–­ç»“æœé€‰æ‹©åˆ†æç­–ç•¥
        if diagnosis['use_alternative_analysis']:
            print("âš ï¸  æ£€æµ‹åˆ°åŸå­æ³¨æ„åŠ›æ¨¡å¼ç›¸åŒï¼Œä½¿ç”¨å…¨å±€åˆ†æç­–ç•¥...\n")
            results['global_analysis'] = self._analyze_global_patterns(
                attention_weights, atoms_object, text_tokens, save_dir, top_k
            )
        else:
            print("âœ… åŸå­æ³¨æ„åŠ›æ¨¡å¼æ­£å¸¸ï¼Œä½¿ç”¨æ ‡å‡†åˆ†æ...\n")
            results['per_atom_analysis'] = self._analyze_per_atom(
                attention_weights, atoms_object, text_tokens, save_dir, top_k
            )

        # æ— è®ºå¦‚ä½•éƒ½åšç»Ÿè®¡åˆ†æ
        results['statistics'] = self._compute_statistics(
            attention_weights, elements, text_tokens
        )

        return results

    def _analyze_global_patterns(self,
                                 attention_weights: Dict[str, torch.Tensor],
                                 atoms_object,
                                 text_tokens: List[str],
                                 save_dir: Optional[Path],
                                 top_k: int) -> Dict:
        """
        å…¨å±€åˆ†æï¼šå½“æ‰€æœ‰åŸå­æ³¨æ„åŠ›ç›¸åŒæ—¶ï¼Œåˆ†ææ•´ä½“æ¨¡å¼
        """
        print("="*80)
        print("ğŸ“Š å…¨å±€æ³¨æ„åŠ›æ¨¡å¼åˆ†æ")
        print("="*80)

        atom_to_text = attention_weights['atom_to_text']
        if isinstance(atom_to_text, torch.Tensor):
            atom_to_text = atom_to_text.cpu().numpy()

        if len(atom_to_text.shape) == 4:
            atom_to_text = atom_to_text[0]

        # å¯¹æ‰€æœ‰åŸå­å’Œæ‰€æœ‰å¤´å–å¹³å‡
        global_attention = atom_to_text.mean(axis=(0, 1))  # [seq_len]

        # è·å–top-kæœ€é‡è¦çš„tokens
        top_indices = global_attention.argsort()[-top_k:][::-1]

        results = {
            'top_tokens': [],
            'token_categories': {},
            'visualization_path': None
        }

        print(f"\nğŸ”¤ å…¨å±€æœ€é‡è¦çš„ {top_k} ä¸ª Tokens:")
        print(f"{'Rank':<6} {'Token':<20} {'Importance':<12} {'Category'}")
        print("-" * 60)

        for rank, idx in enumerate(top_indices, 1):
            if idx < len(text_tokens):
                token = text_tokens[idx]
                importance = global_attention[idx]
                category = self._categorize_token(token)

                results['top_tokens'].append({
                    'token': token,
                    'importance': float(importance),
                    'category': category
                })

                # ç»Ÿè®¡ç±»åˆ«
                if category not in results['token_categories']:
                    results['token_categories'][category] = 0
                results['token_categories'][category] += 1

                print(f"{rank:<6} {token:<20} {importance:<12.6f} {category}")

        # å¯è§†åŒ–
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)

            fig, axes = plt.subplots(2, 2, figsize=(16, 12))

            # 1. Top tokensæŸ±çŠ¶å›¾
            tokens_display = [text_tokens[i] if i < len(text_tokens) else f"[{i}]"
                            for i in top_indices[:10]]
            importances = [global_attention[i] for i in top_indices[:10]]

            axes[0, 0].barh(range(10), importances[::-1], color='steelblue')
            axes[0, 0].set_yticks(range(10))
            axes[0, 0].set_yticklabels(tokens_display[::-1])
            axes[0, 0].set_xlabel('Attention Weight')
            axes[0, 0].set_title('Top 10 Most Important Tokens (Global)', fontweight='bold')
            axes[0, 0].grid(axis='x', alpha=0.3)

            # 2. Tokenç±»åˆ«åˆ†å¸ƒ
            categories = list(results['token_categories'].keys())
            counts = list(results['token_categories'].values())
            axes[0, 1].pie(counts, labels=categories, autopct='%1.1f%%', startangle=90)
            axes[0, 1].set_title('Token Category Distribution', fontweight='bold')

            # 3. æ³¨æ„åŠ›åˆ†å¸ƒçƒ­å›¾ï¼ˆæ‰€æœ‰headså¹³å‡ï¼‰
            avg_per_head = atom_to_text.mean(axis=1)  # [heads, seq_len]
            top_head_idx = avg_per_head.max(axis=1).argmax()

            # æ˜¾ç¤ºæœ€æ´»è·ƒçš„headçš„æ³¨æ„åŠ›
            sns.heatmap(
                atom_to_text[top_head_idx, :, :min(50, len(text_tokens))],
                xticklabels=text_tokens[:min(50, len(text_tokens))],
                yticklabels=[f"{atoms_object.elements[i]}_{i}" for i in range(atom_to_text.shape[1])],
                cmap='YlOrRd',
                ax=axes[1, 0],
                cbar_kws={'label': 'Attention Weight'}
            )
            axes[1, 0].set_title(f'Most Active Head (Head {top_head_idx})', fontweight='bold')
            axes[1, 0].set_xlabel('Text Tokens (first 50)')
            axes[1, 0].set_ylabel('Atoms')
            plt.setp(axes[1, 0].get_xticklabels(), rotation=90, ha='right', fontsize=7)

            # 4. æ³¨æ„åŠ›æƒé‡åˆ†å¸ƒç›´æ–¹å›¾
            all_weights = global_attention.flatten()
            axes[1, 1].hist(all_weights, bins=50, color='coral', alpha=0.7, edgecolor='black')
            axes[1, 1].set_xlabel('Attention Weight')
            axes[1, 1].set_ylabel('Frequency')
            axes[1, 1].set_title('Attention Weight Distribution', fontweight='bold')
            axes[1, 1].axvline(all_weights.mean(), color='red', linestyle='--',
                              label=f'Mean: {all_weights.mean():.4f}')
            axes[1, 1].legend()
            axes[1, 1].grid(alpha=0.3)

            plt.suptitle('Global Attention Pattern Analysis\n(All atoms show similar patterns)',
                        fontsize=14, fontweight='bold')
            plt.tight_layout()

            viz_path = save_dir / 'global_attention_analysis.png'
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()

            results['visualization_path'] = str(viz_path)
            print(f"\nâœ… å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")

        print("="*80 + "\n")
        return results

    def _analyze_per_atom(self,
                         attention_weights: Dict[str, torch.Tensor],
                         atoms_object,
                         text_tokens: List[str],
                         save_dir: Optional[Path],
                         top_k: int) -> Dict:
        """
        é€åŸå­åˆ†æï¼šå½“åŸå­æ³¨æ„åŠ›æ¨¡å¼ä¸åŒæ—¶ä½¿ç”¨
        """
        print("="*80)
        print("âš›ï¸  é€åŸå­æ³¨æ„åŠ›åˆ†æ")
        print("="*80)

        atom_to_text = attention_weights['atom_to_text']
        if isinstance(atom_to_text, torch.Tensor):
            atom_to_text = atom_to_text.cpu().numpy()

        if len(atom_to_text.shape) == 4:
            atom_to_text = atom_to_text[0]

        # å¯¹headså–å¹³å‡
        atom_to_text_avg = atom_to_text.mean(axis=0)  # [num_atoms, seq_len]

        results = {'atoms': {}}

        elements = [str(atoms_object.elements[i]) for i in range(atoms_object.num_atoms)]

        # åˆ†ææ¯ä¸ªåŸå­
        for i, element in enumerate(elements):
            atom_attention = atom_to_text_avg[i]
            top_indices = atom_attention.argsort()[-top_k:][::-1]

            atom_results = {
                'element': element,
                'index': i,
                'top_tokens': []
            }

            for idx in top_indices:
                if idx < len(text_tokens):
                    token = text_tokens[idx]
                    if token.lower() not in self.stopwords:
                        atom_results['top_tokens'].append({
                            'token': token,
                            'weight': float(atom_attention[idx])
                        })

            results['atoms'][f"{element}_{i}"] = atom_results

            # æ‰“å°
            print(f"\n{element}_{i}:")
            for item in atom_results['top_tokens'][:5]:
                print(f"  - {item['token']:<20} {item['weight']:.6f}")

        # å¯è§†åŒ–
        if save_dir:
            save_dir = Path(save_dir)
            save_dir.mkdir(parents=True, exist_ok=True)

            num_atoms = len(elements)
            fig_height = max(8, num_atoms * 0.8)

            fig, ax = plt.subplots(1, 1, figsize=(14, fig_height))

            # åˆ›å»ºçƒ­å›¾æ•°æ®ï¼šæ¯ä¸ªåŸå­çš„top-10 tokens
            max_display = min(10, top_k)
            heatmap_data = np.zeros((num_atoms, max_display))
            token_labels = []

            for i in range(num_atoms):
                atom_key = f"{elements[i]}_{i}"
                top_tokens = results['atoms'][atom_key]['top_tokens'][:max_display]

                for j, item in enumerate(top_tokens):
                    heatmap_data[i, j] = item['weight']
                    if i == 0:  # åªåœ¨ç¬¬ä¸€è¡Œè®°å½•tokenåç§°
                        token_labels.append(item['token'])

            # å¡«å……tokenæ ‡ç­¾
            while len(token_labels) < max_display:
                token_labels.append('')

            sns.heatmap(
                heatmap_data,
                xticklabels=token_labels,
                yticklabels=[f"{elements[i]}_{i}" for i in range(num_atoms)],
                cmap='YlOrRd',
                ax=ax,
                annot=True,
                fmt='.4f',
                cbar_kws={'label': 'Attention Weight'}
            )

            ax.set_title(f'Per-Atom Top {max_display} Attended Tokens', fontweight='bold', fontsize=14)
            ax.set_xlabel('Top Tokens', fontsize=11)
            ax.set_ylabel('Atoms', fontsize=11)
            plt.setp(ax.get_xticklabels(), rotation=45, ha='right')

            plt.tight_layout()

            viz_path = save_dir / 'per_atom_attention.png'
            plt.savefig(viz_path, dpi=300, bbox_inches='tight')
            plt.close()

            results['visualization_path'] = str(viz_path)
            print(f"\nâœ… å¯è§†åŒ–å·²ä¿å­˜: {viz_path}")

        print("="*80 + "\n")
        return results

    def _compute_statistics(self,
                           attention_weights: Dict[str, torch.Tensor],
                           elements: List[str],
                           text_tokens: List[str]) -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""

        atom_to_text = attention_weights['atom_to_text']
        if isinstance(atom_to_text, torch.Tensor):
            atom_to_text = atom_to_text.cpu().numpy()

        if len(atom_to_text.shape) == 4:
            atom_to_text = atom_to_text[0]

        stats = {
            'num_heads': atom_to_text.shape[0],
            'num_atoms': atom_to_text.shape[1],
            'seq_len': atom_to_text.shape[2],
            'mean_attention': float(atom_to_text.mean()),
            'std_attention': float(atom_to_text.std()),
            'max_attention': float(atom_to_text.max()),
            'min_attention': float(atom_to_text.min()),
            'sparsity': float((atom_to_text < 0.01).sum() / atom_to_text.size)
        }

        return stats

    def _categorize_token(self, token: str) -> str:
        """å°†tokenåˆ†ç±»"""
        token_lower = token.lower().replace('##', '')

        # å…ƒç´ ç¬¦å·
        elements = {'h', 'he', 'li', 'be', 'b', 'c', 'n', 'o', 'f', 'ne',
                   'na', 'mg', 'al', 'si', 'p', 's', 'cl', 'ar', 'k', 'ca',
                   'ba', 'hf', 'ti', 'zr', 'nb', 'mo', 'tc', 'ru', 'rh', 'pd'}

        # æ•°å­—
        if token_lower.isdigit():
            return 'Number'

        # å…ƒç´ 
        if token_lower in elements or any(token_lower.startswith(e) for e in elements):
            return 'Element'

        # æ™¶ä½“å­¦æœ¯è¯­
        crystal_terms = {'cubic', 'tetragonal', 'orthorhombic', 'monoclinic',
                        'triclinic', 'hexagonal', 'rhombohedral', 'space', 'group',
                        'coordinate', 'symmetry', 'lattice', 'framework'}
        if any(term in token_lower for term in crystal_terms):
            return 'Crystallography'

        # åŒ–å­¦æœ¯è¯­
        chem_terms = {'bond', 'atom', 'molecule', 'cluster', 'ion', 'electron',
                     'oxidation', 'valence', 'coordination'}
        if any(term in token_lower for term in chem_terms):
            return 'Chemistry'

        # åœç”¨è¯
        if token_lower in self.stopwords:
            return 'Stopword'

        return 'Other'


def run_complete_analysis(model, g, lg, text, atoms_object, save_dir=None):
    """
    è¿è¡Œå®Œæ•´çš„æ³¨æ„åŠ›åˆ†æ

    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        g, lg: DGLå›¾
        text: æ–‡æœ¬æè¿°
        atoms_object: JARVIS Atomså¯¹è±¡
        save_dir: ä¿å­˜ç›®å½•
    """
    from transformers import BertTokenizer

    device = next(model.parameters()).device
    g = g.to(device)
    lg = lg.to(device)

    # è·å–é¢„æµ‹å’Œæ³¨æ„åŠ›
    with torch.no_grad():
        output = model(
            [g, lg, [text]],
            return_features=True,
            return_attention=True
        )

    prediction = output['predictions'].cpu().item()
    fg_attn = output.get('fine_grained_attention_weights', None)

    if fg_attn is None:
        print("âŒ æ¨¡å‹æœªè¿”å› fine-grained attention weights")
        return None

    print(f"\nâœ… é¢„æµ‹å€¼: {prediction:.4f}")

    # Tokenizeæ–‡æœ¬
    tokenizer = BertTokenizer.from_pretrained('m3rg-iitd/matscibert')
    tokens = tokenizer.tokenize(text)
    tokens = ['[CLS]'] + tokens + ['[SEP]']

    # å¯¹é½é•¿åº¦
    seq_len = fg_attn['atom_to_text'].shape[-1]
    if len(tokens) > seq_len:
        tokens = tokens[:seq_len]
    elif len(tokens) < seq_len:
        tokens = tokens + ['[PAD]'] * (seq_len - len(tokens))

    # åˆ›å»ºåˆ†æå™¨å¹¶è¿è¡Œ
    analyzer = RobustAttentionAnalyzer(model, device)
    results = analyzer.analyze_with_fallback(
        fg_attn,
        atoms_object,
        tokens,
        save_dir=save_dir,
        top_k=15
    )

    return results


if __name__ == '__main__':
    print("Robust Attention Analyzer - å¯ç‹¬ç«‹è¿è¡Œæˆ–ä½œä¸ºæ¨¡å—å¯¼å…¥")
    print("ä½¿ç”¨ run_complete_analysis() å‡½æ•°è¿›è¡Œå®Œæ•´åˆ†æ")
