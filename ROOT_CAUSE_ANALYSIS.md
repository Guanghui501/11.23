# Fine-Grained Attention é—®é¢˜ï¼šæ ¹æœ¬åŸå› åˆ†æä¸è§£å†³æ–¹æ¡ˆ

## ğŸ“‹ é—®é¢˜æ€»ç»“

**ç—‡çŠ¶**ï¼šæ‰€æœ‰åŸå­ï¼ˆBa_0, Ba_1, Ba_2, Ba_3, Hf_4, Li_5ï¼‰æ˜¾ç¤ºå®Œå…¨ç›¸åŒçš„ Top Words

**è¯Šæ–­ç»“æœ**ï¼š
- âœ… ä»£ç é€»è¾‘å®Œå…¨æ­£ç¡®ï¼ˆæ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼‰
- âŒ æ¨¡å‹è¾“å‡ºçš„ fine-grained attention å¯¹æ‰€æœ‰åŸå­å®Œå…¨ç›¸åŒ
- âŒ ç›¸å…³ç³»æ•° = 1.0ï¼Œæ–¹å·® = 0.0
- âŒ æ‰€æœ‰åŸå­å…³æ³¨åŒä¸€ä¸ª token (index=2)

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

### é—®é¢˜æ‰€åœ¨ï¼š**GNN å±‚è¾“å‡ºçš„åŸå­ç‰¹å¾ç›¸åŒ**

é€šè¿‡åˆ†æ `models/alignn.py` ä»£ç æµç¨‹ï¼š

```python
# Line 920: GNNå¤„ç†åçš„èŠ‚ç‚¹ç‰¹å¾
x = [ç»è¿‡ALIGNNå±‚å¤„ç†çš„èŠ‚ç‚¹ç‰¹å¾]  # [total_atoms, node_dim]

# Line 941: æå–æ¯ä¸ªåŸå­çš„ç‰¹å¾ç”¨äºfine-grained attention
node_features_batched[i, :num_nodes] = x[offset:offset+num_nodes]

# Line 455: åŸºäºåŸå­ç‰¹å¾ç”ŸæˆattentionæŸ¥è¯¢
Q_a2t = self.a2t_query(node_feat)  # å¦‚æœnode_featç›¸åŒï¼ŒQ_a2tä¹Ÿç›¸åŒ

# Line 465-473: è®¡ç®—attention weights
attn_a2t = torch.matmul(Q_a2t, K_a2t.transpose(-2, -1)) * self.scale
attn_a2t = F.softmax(attn_a2t, dim=-1)
```

**å…³é”®æ¨è®º**ï¼š
å¦‚æœ GNN è¾“å‡ºçš„åŸå­ç‰¹å¾ `x` å¯¹æ‰€æœ‰åŸå­éƒ½ç›¸åŒæˆ–éå¸¸ç›¸ä¼¼ï¼Œé‚£ä¹ˆï¼š
1. æ‰€æœ‰åŸå­çš„ Query å‘é‡ `Q_a2t` å°†ç›¸åŒ
2. æ‰€æœ‰åŸå­çš„ attention scores å°†ç›¸åŒ
3. æ‰€æœ‰åŸå­çš„ attention weights å°†å®Œå…¨ä¸€è‡´

### ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿ

#### å¯èƒ½åŸå›  1: **GNN è¿‡åº¦å¹³æ»‘ (Over-smoothing)**

GNN çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜ï¼šç»è¿‡å¤šå±‚ä¼ æ’­åï¼Œæ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾è¶‹å‘äºæ”¶æ•›åˆ°ç›¸åŒçš„å€¼ã€‚

**è¯æ®**ï¼š
- æ‚¨çš„æ¨¡å‹é…ç½®ï¼š`alignn_layers=4, gcn_layers=4`
- æ€»å…± 8 å±‚ GNN ä¼ æ’­
- å¯¹äºå°åˆ†å­ï¼ˆ6ä¸ªåŸå­ï¼‰ï¼Œè¿‡åº¦å¹³æ»‘æ›´å®¹æ˜“å‘ç”Ÿ

**ç†è®ºè§£é‡Š**ï¼š
æ¯å±‚ GNN ä¼šèšåˆé‚»å±…ä¿¡æ¯ï¼Œå¤šå±‚åæ‰€æœ‰èŠ‚ç‚¹"çœ‹åˆ°"ç›¸ä¼¼çš„å…¨å±€ä¿¡æ¯ã€‚

#### å¯èƒ½åŸå›  2: **Fine-Grained Attention å±‚æœªè¢«è®­ç»ƒ**

æ£€æŸ¥ç‚¹ï¼š
```python
use_fine_grained_attention: True  âœ… (é…ç½®æ­£ç¡®)
```

ä½†æ˜¯ï¼Œæ¨¡å‹å¯èƒ½ï¼š
- è®­ç»ƒæ—¶æ²¡æœ‰ä½¿ç”¨ fine-grained attention çš„ç›‘ç£ä¿¡å·
- æˆ–è€…è®­ç»ƒæ—¶ `use_fine_grained_attention=False`ï¼Œåªæ˜¯æ¨ç†æ—¶æ‰“å¼€
- å‚æ•°å¤„äºéšæœºåˆå§‹åŒ–çŠ¶æ€æˆ–å…¨é›¶çŠ¶æ€

#### å¯èƒ½åŸå›  3: **Middle Fusion çš„å½±å“**

æ‚¨çš„è¾“å‡ºæ˜¾ç¤ºï¼š
```python
ğŸ” MiddleFusionModule.forward è°ƒè¯•:
   node_feat.shape: torch.Size([6, 256])
   text_feat.shape: torch.Size([1, 64])
   batch_num_nodes: [6]
   text_transformed.shape: torch.Size([1, 256])
   text_broadcasted.shape: torch.Size([6, 256])
   gate_input.shape: torch.Size([6, 512])
```

Middle Fusion åœ¨ fine-grained attention **ä¹‹å‰**èåˆæ–‡æœ¬ä¿¡æ¯åˆ°èŠ‚ç‚¹ç‰¹å¾ã€‚å¦‚æœè¿™ä¸ªèåˆæ“ä½œå¯¼è‡´æ‰€æœ‰èŠ‚ç‚¹ç‰¹å¾å˜å¾—ç›¸åŒï¼Œå°±ä¼šæœ‰é—®é¢˜ã€‚

## ğŸ”§ è¯Šæ–­æ­¥éª¤

### æ­¥éª¤ 1: æ£€æŸ¥ GNN è¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦ç›¸åŒ

åœ¨ `models/alignn.py` çš„ç¬¬ 920 è¡Œåæ·»åŠ è¯Šæ–­ä»£ç ï¼š

```python
# åœ¨è¿™ä¸€è¡Œä¹‹åï¼š
# temp_graph_emb = self.readout(g, x)

# æ·»åŠ è¯Šæ–­ï¼š
if return_attention and self.use_fine_grained_attention:
    print(f"\nğŸ” è¯Šæ–­ GNN è¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾:")
    batch_num_nodes = g.batch_num_nodes().tolist()
    offset = 0
    for i, num_nodes in enumerate(batch_num_nodes):
        node_feats = x[offset:offset+num_nodes]  # [num_atoms, node_dim]
        print(f"  Graph {i}: {num_nodes} atoms")

        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦ç›¸åŒ
        if num_nodes > 1:
            feat_0 = node_feats[0].cpu().numpy()
            feat_1 = node_feats[1].cpu().numpy()
            correlation = np.corrcoef(feat_0, feat_1)[0, 1]
            identical = torch.allclose(node_feats[0], node_feats[1], atol=1e-6)

            print(f"    Atom 0 vs Atom 1 correlation: {correlation:.6f}")
            print(f"    Identical (atol=1e-6): {identical}")

            # æ£€æŸ¥æ‰€æœ‰åŸå­çš„æ–¹å·®
            feats_np = node_feats.cpu().numpy()
            atom_means = feats_np.mean(axis=1)  # [num_atoms]
            variance = atom_means.var()
            print(f"    Variance across atoms: {variance:.6f}")

            if identical or correlation > 0.99:
                print(f"    âš ï¸  é—®é¢˜ç¡®è®¤ï¼šGNNè¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾å‡ ä¹ç›¸åŒ!")
            else:
                print(f"    âœ… GNNè¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾æœ‰å·®å¼‚")

        offset += num_nodes
```

### æ­¥éª¤ 2: æ£€æŸ¥è®­ç»ƒé…ç½®

æ£€æŸ¥æ‚¨çš„è®­ç»ƒè„šæœ¬ï¼Œç¡®è®¤ï¼š

```python
# è®­ç»ƒæ—¶æ˜¯å¦å¯ç”¨äº† fine-grained attentionï¼Ÿ
config = ALIGNNConfig(
    ...
    use_fine_grained_attention=True,  # â† å¿…é¡»æ˜¯ True
    use_middle_fusion=True,
    ...
)

# æ˜¯å¦æœ‰é’ˆå¯¹ fine-grained attention çš„æŸå¤±å‡½æ•°ï¼Ÿ
# å¦‚æœåªæœ‰ä¸»ä»»åŠ¡æŸå¤±ï¼ˆå¦‚MAEï¼‰ï¼Œfine-grained attentionå¯èƒ½ä¸ä¼šå­¦åˆ°æœ‰ç”¨çš„æ¨¡å¼
```

### æ­¥éª¤ 3: æ£€æŸ¥ Checkpoint åŠ è½½

éªŒè¯checkpointç¡®å®åŒ…å« fine-grained attention çš„æƒé‡ï¼š

```python
checkpoint = torch.load(checkpoint_path)
state_dict = checkpoint.get('model', checkpoint)

# æ£€æŸ¥æ˜¯å¦æœ‰ fine-grained attention çš„æƒé‡
fg_keys = [k for k in state_dict.keys() if 'fine_grained' in k]
print(f"Fine-grained attention keys: {len(fg_keys)}")
for key in fg_keys[:5]:
    print(f"  {key}: {state_dict[key].shape}")

# æ£€æŸ¥æƒé‡æ˜¯å¦ä¸ºé›¶æˆ–éšæœº
if fg_keys:
    first_key = fg_keys[0]
    weight = state_dict[first_key]
    print(f"\næƒé‡ç»Ÿè®¡:")
    print(f"  Mean: {weight.mean():.6f}")
    print(f"  Std: {weight.std():.6f}")
    print(f"  å…¨é›¶?: {torch.allclose(weight, torch.zeros_like(weight))}")
```

## ğŸ’¡ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1: **ç¦ç”¨ Middle Fusion**ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

Middle fusion å¯èƒ½å¯¼è‡´èŠ‚ç‚¹ç‰¹å¾åŒè´¨åŒ–ã€‚å°è¯•ï¼š

```python
config = ALIGNNConfig(
    ...
    use_middle_fusion=False,  # ç¦ç”¨
    use_fine_grained_attention=True,  # ä¿ç•™
    ...
)
```

é‡æ–°è¿è¡Œè¯Šæ–­ï¼Œçœ‹ GNN è¾“å‡ºçš„èŠ‚ç‚¹ç‰¹å¾æ˜¯å¦æœ‰å·®å¼‚ã€‚

### æ–¹æ¡ˆ 2: **å‡å°‘ GNN å±‚æ•°**ï¼ˆç¼“è§£è¿‡åº¦å¹³æ»‘ï¼‰

```python
config = ALIGNNConfig(
    alignn_layers=2,  # ä» 4 å‡å°‘åˆ° 2
    gcn_layers=2,      # ä» 4 å‡å°‘åˆ° 2
    ...
)
```

æ³¨æ„ï¼šéœ€è¦é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚

### æ–¹æ¡ˆ 3: **æ·»åŠ æ®‹å·®è¿æ¥å’Œå½’ä¸€åŒ–**

åœ¨ GNN å±‚ä¸­æ·»åŠ æ›´å¼ºçš„æ®‹å·®è¿æ¥ï¼Œé˜²æ­¢è¿‡åº¦å¹³æ»‘ã€‚è¿™éœ€è¦ä¿®æ”¹æ¨¡å‹æ¶æ„ã€‚

### æ–¹æ¡ˆ 4: **é‡æ–°è®­ç»ƒæ¨¡å‹**

å¦‚æœ checkpoint ç¡®å®æ²¡æœ‰æ­£ç¡®è®­ç»ƒ fine-grained attentionï¼Œéœ€è¦ï¼š

1. **å¯ç”¨ fine-grained attention ç›‘ç£**ï¼š
   - æ·»åŠ æ³¨æ„åŠ›æ­£åˆ™åŒ–æŸå¤±
   - æˆ–ä½¿ç”¨å¯¹æ¯”å­¦ä¹ é¼“åŠ±ä¸åŒåŸå­å…³æ³¨ä¸åŒè¯

2. **è®­ç»ƒé…ç½®**ï¼š
```python
# ç¡®ä¿è®­ç»ƒæ—¶å¯ç”¨
use_fine_grained_attention=True

# å¯èƒ½çš„æŸå¤±å‡½æ•°è®¾è®¡ï¼š
# loss = mae_loss + lambda * diversity_loss
# diversity_loss = -variance(attention_weights_per_atom)  # é¼“åŠ±å·®å¼‚åŒ–
```

### æ–¹æ¡ˆ 5: **ä½¿ç”¨é¢„è®­ç»ƒçš„åŸå­åµŒå…¥**

ä½¿ç”¨åŸå­ç±»å‹ç‰¹å®šçš„é¢„è®­ç»ƒåµŒå…¥ï¼ˆå¦‚åŸå­åºæ•°ã€ç”µè´Ÿæ€§ç­‰ï¼‰ï¼Œç¡®ä¿ä¸åŒåŸå­æœ‰ä¸åŒçš„åˆå§‹ç‰¹å¾ã€‚

```python
# åœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶
self.atom_embedding = nn.Embedding(103, embedding_dim)  # 103ç§å…ƒç´ 
# åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œç¡®ä¿Ba, Hf, Liæœ‰ä¸åŒçš„åµŒå…¥
```

## ğŸ“Š ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œï¼ˆè¯Šæ–­ï¼‰ï¼š

1. **è¿è¡Œæ­¥éª¤ 1 çš„è¯Šæ–­ä»£ç **ï¼Œç¡®è®¤ GNN è¾“å‡ºæ˜¯å¦ç›¸åŒ
2. **è¿è¡Œæ­¥éª¤ 3**ï¼Œæ£€æŸ¥ checkpoint æƒé‡
3. **å°†è¯Šæ–­ç»“æœå‘Šè¯‰æˆ‘**

### å¿«é€ŸéªŒè¯ï¼ˆæµ‹è¯•ï¼‰ï¼š

1. **å°è¯•æ–¹æ¡ˆ 1**ï¼ˆç¦ç”¨ middle fusionï¼‰ï¼Œçœ‹æ˜¯å¦æ”¹å–„
2. **æµ‹è¯•ä¸åŒæ ·æœ¬**ï¼Œç¡®è®¤æ˜¯æ™®éé—®é¢˜è¿˜æ˜¯ä¸ªä¾‹

### é•¿æœŸæ–¹æ¡ˆï¼ˆéœ€é‡æ–°è®­ç»ƒï¼‰ï¼š

1. æ–¹æ¡ˆ 2 æˆ– 4ï¼Œæ ¹æ®è¯Šæ–­ç»“æœå†³å®š
2. é‡æ–°è®­ç»ƒæ—¶ç›‘æ§ attention diversity æŒ‡æ ‡

## ğŸ“ å‚è€ƒèµ„æ–™

**GNN Over-smoothing**ï¼š
- [Understanding and Resolving Performance Degradation in Graph Convolutional Networks](https://arxiv.org/abs/1911.10797)
- [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning](https://arxiv.org/abs/1801.07606)

**Attention Diversity**ï¼š
- [Are Sixteen Heads Really Better than One?](https://arxiv.org/abs/1905.10650)
- å¤šå¤´æ³¨æ„åŠ›çš„å¤šæ ·æ€§å¯¹æ¨¡å‹æ€§èƒ½å¾ˆé‡è¦

---

**æ€»ç»“**ï¼šé—®é¢˜ä¸åœ¨å¯è§†åŒ–ä»£ç ï¼Œè€Œåœ¨äºæ¨¡å‹æœ¬èº«ã€‚GNN è¾“å‡ºçš„åŸå­ç‰¹å¾å¯èƒ½è¿‡äºç›¸ä¼¼ï¼Œå¯¼è‡´ fine-grained attention æ— æ³•åŒºåˆ†ä¸åŒåŸå­ã€‚éœ€è¦é€šè¿‡è¯Šæ–­ç¡®è®¤ï¼Œç„¶åé‡‡å–ç›¸åº”è§£å†³æ–¹æ¡ˆã€‚
