#!/usr/bin/env python
"""
éš¾åˆ†æ ·æœ¬å¯è§†åŒ– (Hard Class Subset) - å¹³è¡¡ç‰ˆæœ¬
åœ¨åŸç‰ˆåŸºç¡€ä¸Šå¢åŠ æ ·æœ¬å¹³è¡¡åŠŸèƒ½
"""

import os
import sys
import argparse
import numpy as np
import random

# å¯¼å…¥åŸå§‹è„šæœ¬çš„æ‰€æœ‰å‡½æ•°
sys.path.insert(0, os.path.dirname(__file__))
from visualize_hard_class_subset import (
    load_model, extract_crystal_systems_from_dataset,
    filter_by_crystal_systems, extract_features,
    compute_clustering_metrics, compute_class_separation,
    apply_reduction, plot_hard_class_comparison,
    plot_separation_metrics, CRYSTAL_SYSTEMS
)

from pathlib import Path
from tqdm import tqdm
import torch
from jarvis.core.atoms import Atoms
from data import get_torch_dataset
from torch.utils.data import DataLoader


def balance_samples_by_class(filtered_dataset, filtered_systems, target_systems, balance_method='downsample'):
    """
    å¹³è¡¡ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡

    Args:
        filtered_dataset: è¿‡æ»¤åçš„æ•°æ®é›†
        filtered_systems: è¿‡æ»¤åçš„æ™¶ç³»åˆ—è¡¨
        target_systems: ç›®æ ‡æ™¶ç³»åˆ—è¡¨ [class1, class2]
        balance_method: 'downsample' (ä¸‹é‡‡æ ·å¤šæ•°ç±») æˆ– 'upsample' (ä¸Šé‡‡æ ·å°‘æ•°ç±»)

    Returns:
        balanced_dataset: å¹³è¡¡åçš„æ•°æ®é›†
        balanced_systems: å¹³è¡¡åçš„æ™¶ç³»åˆ—è¡¨
    """
    class1, class2 = target_systems

    # åˆ†ç¦»ä¸¤ä¸ªç±»åˆ«
    class1_indices = [i for i, cs in enumerate(filtered_systems) if cs == class1]
    class2_indices = [i for i, cs in enumerate(filtered_systems) if cs == class2]

    n1 = len(class1_indices)
    n2 = len(class2_indices)

    print(f"\nğŸ”„ æ ·æœ¬å¹³è¡¡ (æ–¹æ³•: {balance_method})")
    print(f"   åŸå§‹æ ·æœ¬æ•°:")
    print(f"     {CRYSTAL_SYSTEMS[class1]}: {n1}")
    print(f"     {CRYSTAL_SYSTEMS[class2]}: {n2}")

    if balance_method == 'downsample':
        # ä¸‹é‡‡æ ·åˆ°è¾ƒå°çš„ç±»åˆ«æ•°é‡
        target_size = min(n1, n2)

        if n1 > target_size:
            random.shuffle(class1_indices)
            class1_indices = class1_indices[:target_size]

        if n2 > target_size:
            random.shuffle(class2_indices)
            class2_indices = class2_indices[:target_size]

    elif balance_method == 'upsample':
        # ä¸Šé‡‡æ ·åˆ°è¾ƒå¤§çš„ç±»åˆ«æ•°é‡
        target_size = max(n1, n2)

        if n1 < target_size:
            # é‡å¤é‡‡æ ·
            class1_indices = class1_indices + random.choices(class1_indices, k=target_size - n1)

        if n2 < target_size:
            class2_indices = class2_indices + random.choices(class2_indices, k=target_size - n2)

    else:
        raise ValueError(f"Unknown balance method: {balance_method}")

    # åˆå¹¶å¹¶é‡æ–°æ„å»ºæ•°æ®é›†
    balanced_indices = class1_indices + class2_indices
    random.shuffle(balanced_indices)

    balanced_dataset = [filtered_dataset[i] for i in balanced_indices]
    balanced_systems = [filtered_systems[i] for i in balanced_indices]

    # ç»Ÿè®¡å¹³è¡¡åçš„æ•°é‡
    n1_balanced = balanced_systems.count(class1)
    n2_balanced = balanced_systems.count(class2)

    print(f"   å¹³è¡¡åæ ·æœ¬æ•°:")
    print(f"     {CRYSTAL_SYSTEMS[class1]}: {n1_balanced}")
    print(f"     {CRYSTAL_SYSTEMS[class2]}: {n2_balanced}")
    print(f"   æ€»æ ·æœ¬æ•°: {len(balanced_dataset)}")

    return balanced_dataset, balanced_systems


def main():
    parser = argparse.ArgumentParser(description='éš¾åˆ†æ ·æœ¬å¯è§†åŒ– (å¹³è¡¡ç‰ˆæœ¬)')
    parser.add_argument('--checkpoint_without_fusion', type=str, required=True)
    parser.add_argument('--checkpoint_with_fusion', type=str, required=True)
    parser.add_argument('--data_dir', type=str, required=True)
    parser.add_argument('--dataset', type=str, default='jarvis')
    parser.add_argument('--property', type=str, default='mbj_bandgap')
    parser.add_argument('--class_pair', type=str, default='cubic,tetragonal')
    parser.add_argument('--n_samples', type=int, default=2000)
    parser.add_argument('--balance_method', type=str, default='downsample',
                       choices=['downsample', 'upsample'],
                       help='å¹³è¡¡æ–¹æ³•: downsample(ä¸‹é‡‡æ ·) æˆ– upsample(ä¸Šé‡‡æ ·)')
    parser.add_argument('--reduction_method', type=str, default='tsne',
                       choices=['tsne', 'umap'])
    parser.add_argument('--output_dir', type=str, default='hard_class_balanced_results')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')

    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    random.seed(42)
    np.random.seed(42)

    # è§£ææ™¶ç³»å¯¹
    class_pair = [cs.strip().lower() for cs in args.class_pair.split(',')]
    if len(class_pair) != 2:
        raise ValueError("--class_pair å¿…é¡»æŒ‡å®šä¸¤ä¸ªæ™¶ç³»")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("éš¾åˆ†æ ·æœ¬å¯è§†åŒ– (å¹³è¡¡ç‰ˆæœ¬)")
    print("=" * 80)
    print(f"ç›®æ ‡æ™¶ç³»å¯¹: {CRYSTAL_SYSTEMS[class_pair[0]]} vs {CRYSTAL_SYSTEMS[class_pair[1]]}")
    print(f"å¹³è¡¡æ–¹æ³•: {args.balance_method}")
    print("=" * 80)

    # æ„å»ºè·¯å¾„
    cif_dir = os.path.join(args.data_dir, f'{args.dataset}/{args.property}/cif/')
    id_prop_file = os.path.join(args.data_dir, f'{args.dataset}/{args.property}/description.csv')

    # åŠ è½½æ¨¡å‹
    print("\n" + "=" * 80)
    print("1ï¸âƒ£ åŠ è½½æ¨¡å‹")
    print("=" * 80)
    model_without, _ = load_model(args.checkpoint_without_fusion, args.device)
    model_with, _ = load_model(args.checkpoint_with_fusion, args.device)

    # åŠ è½½æ•°æ®
    print("\n" + "=" * 80)
    print("2ï¸âƒ£ åŠ è½½æ•°æ®é›†")
    print("=" * 80)

    import csv
    with open(id_prop_file, 'r') as f:
        reader = csv.reader(f)
        headings = next(reader)
        data = [row for row in reader]

    print(f"æ€»æ ·æœ¬æ•°: {len(data)}")

    # é‡‡æ ·
    if len(data) > args.n_samples:
        random.shuffle(data)
        data = data[:args.n_samples]
        print(f"éšæœºé‡‡æ · {args.n_samples} ä¸ªæ ·æœ¬")

    # æ„å»ºdataset_array
    dataset_array = []
    skipped = 0

    for j in tqdm(range(len(data)), desc="åŠ è½½æ ·æœ¬"):
        try:
            sample_id = data[j][0]
            target_val = float(data[j][2])
            description = data[j][3] if len(data[j]) > 3 else ""

            cif_file = os.path.join(cif_dir, f'{sample_id}.cif')
            if not os.path.exists(cif_file):
                skipped += 1
                continue

            atoms = Atoms.from_cif(cif_file)

            info = {
                "atoms": atoms.to_dict(),
                "jid": sample_id,
                "text": description if description else atoms.composition.reduced_formula,
                "target": target_val
            }

            dataset_array.append(info)

        except Exception as e:
            skipped += 1

    print(f"âœ“ æˆåŠŸåŠ è½½: {len(dataset_array)} æ ·æœ¬, è·³è¿‡: {skipped} æ ·æœ¬")

    # æå–æ™¶ç³»
    print("\n" + "=" * 80)
    print("3ï¸âƒ£ æå–æ™¶ç³»ä¿¡æ¯")
    print("=" * 80)
    crystal_systems, sample_ids = extract_crystal_systems_from_dataset(dataset_array, cif_dir)

    # ç­›é€‰ç›®æ ‡æ™¶ç³»
    print("\n" + "=" * 80)
    print("4ï¸âƒ£ ç­›é€‰ç›®æ ‡æ™¶ç³»")
    print("=" * 80)
    filtered_dataset, filtered_systems, filtered_indices = filter_by_crystal_systems(
        dataset_array, crystal_systems, class_pair
    )

    # å¹³è¡¡æ ·æœ¬æ•°é‡
    print("\n" + "=" * 80)
    print("5ï¸âƒ£ å¹³è¡¡æ ·æœ¬æ•°é‡")
    print("=" * 80)
    balanced_dataset, balanced_systems = balance_samples_by_class(
        filtered_dataset, filtered_systems, class_pair, args.balance_method
    )

    if len(balanced_dataset) < 10:
        print(f"âŒ é”™è¯¯: å¹³è¡¡åçš„æ ·æœ¬æ•°å¤ªå°‘ ({len(balanced_dataset)})ï¼Œæ— æ³•è¿›è¡Œåˆ†æ")
        return

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("\n" + "=" * 80)
    print("6ï¸âƒ£ åˆ›å»ºæ•°æ®åŠ è½½å™¨")
    print("=" * 80)

    test_data = get_torch_dataset(
        dataset=balanced_dataset,
        id_tag="jid",
        target="target",
        neighbor_strategy="k-nearest",
        atom_features="cgcnn",
        use_canonize=False,
        name=f"{args.dataset}_{args.property}_balanced",
        line_graph=True,
        cutoff=8.0,
        max_neighbors=12,
    )

    test_loader = DataLoader(
        test_data,
        batch_size=32,
        shuffle=False,
        num_workers=0,
        pin_memory=False,
        collate_fn=test_data.collate_line_graph,
    )

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {len(test_data)} æ ·æœ¬")

    # æå–ç‰¹å¾
    print("\n" + "=" * 80)
    print("7ï¸âƒ£ æå–ç‰¹å¾")
    print("=" * 80)

    print("æ— ä¸­æœŸèåˆæ¨¡å‹:")
    features_without, targets = extract_features(model_without, test_loader, args.device)

    print("\næœ‰ä¸­æœŸèåˆæ¨¡å‹:")
    features_with, _ = extract_features(model_with, test_loader, args.device)

    # è®¡ç®—èšç±»æŒ‡æ ‡
    print("\n" + "=" * 80)
    print("8ï¸âƒ£ è®¡ç®—èšç±»æŒ‡æ ‡")
    print("=" * 80)

    metrics_without = compute_clustering_metrics(features_without, balanced_systems)
    metrics_with = compute_clustering_metrics(features_with, balanced_systems)

    print(f"\næ— ä¸­æœŸèåˆ:")
    for k, v in metrics_without.items():
        print(f"  {k}: {v:.4f}")

    print(f"\næœ‰ä¸­æœŸèåˆ:")
    for k, v in metrics_with.items():
        print(f"  {k}: {v:.4f}")

    # è®¡ç®—ç±»åˆ†ç¦»åº¦
    print("\n" + "=" * 80)
    print("9ï¸âƒ£ è®¡ç®—ç±»åˆ†ç¦»åº¦")
    print("=" * 80)

    sep_without = compute_class_separation(features_without, balanced_systems, class_pair[0], class_pair[1])
    sep_with = compute_class_separation(features_with, balanced_systems, class_pair[0], class_pair[1])

    print(f"\næ— ä¸­æœŸèåˆ:")
    print(f"  ç±»é—´è·ç¦»: {sep_without['inter_class_dist']:.4f}")
    print(f"  ç±»å†…è·ç¦»1 ({class_pair[0]}): {sep_without['intra_class_dist_1']:.4f}")
    print(f"  ç±»å†…è·ç¦»2 ({class_pair[1]}): {sep_without['intra_class_dist_2']:.4f}")
    print(f"  åˆ†ç¦»æ¯”ç‡: {sep_without['separation_ratio']:.4f}")

    print(f"\næœ‰ä¸­æœŸèåˆ:")
    print(f"  ç±»é—´è·ç¦»: {sep_with['inter_class_dist']:.4f}")
    print(f"  ç±»å†…è·ç¦»1 ({class_pair[0]}): {sep_with['intra_class_dist_1']:.4f}")
    print(f"  ç±»å†…è·ç¦»2 ({class_pair[1]}): {sep_with['intra_class_dist_2']:.4f}")
    print(f"  åˆ†ç¦»æ¯”ç‡: {sep_with['separation_ratio']:.4f}")

    # é™ç»´
    print("\n" + "=" * 80)
    print("ğŸ”Ÿ é™ç»´å¯è§†åŒ–")
    print("=" * 80)

    embedded_without = apply_reduction(features_without, method=args.reduction_method, n_components=2)
    embedded_with = apply_reduction(features_with, method=args.reduction_method, n_components=2)

    # å¯è§†åŒ–
    print("\n" + "=" * 80)
    print("1ï¸âƒ£1ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾åƒ")
    print("=" * 80)

    comparison_path = output_dir / f"hard_class_{class_pair[0]}_vs_{class_pair[1]}_balanced.png"
    plot_hard_class_comparison(embedded_without, embedded_with, balanced_systems,
                               metrics_without, metrics_with,
                               sep_without, sep_with,
                               class_pair, comparison_path)

    separation_path = output_dir / f"separation_metrics_{class_pair[0]}_vs_{class_pair[1]}_balanced.png"
    plot_separation_metrics(sep_without, sep_with, class_pair, separation_path)

    # ä¿å­˜ç»“æœæ‘˜è¦
    summary_path = output_dir / f"summary_{class_pair[0]}_vs_{class_pair[1]}_balanced.txt"
    with open(summary_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("éš¾åˆ†æ ·æœ¬å¯è§†åŒ–åˆ†æç»“æœ (å¹³è¡¡ç‰ˆæœ¬)\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ•°æ®é›†: {args.dataset} - {args.property}\n")
        f.write(f"æ™¶ç³»å¯¹: {CRYSTAL_SYSTEMS[class_pair[0]]} vs {CRYSTAL_SYSTEMS[class_pair[1]]}\n")
        f.write(f"å¹³è¡¡æ–¹æ³•: {args.balance_method}\n")
        f.write(f"å¹³è¡¡åæ ·æœ¬æ•°: {len(balanced_systems)}\n")
        f.write(f"  {CRYSTAL_SYSTEMS[class_pair[0]]}: {balanced_systems.count(class_pair[0])}\n")
        f.write(f"  {CRYSTAL_SYSTEMS[class_pair[1]]}: {balanced_systems.count(class_pair[1])}\n")
        f.write(f"é™ç»´æ–¹æ³•: {args.reduction_method.upper()}\n\n")

        f.write("=" * 80 + "\n")
        f.write("èšç±»æŒ‡æ ‡å¯¹æ¯”\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"{'æŒ‡æ ‡':<30} {'æ— èåˆ':<15} {'æœ‰èåˆ':<15} {'æ”¹è¿›':<15}\n")
        f.write("-" * 80 + "\n")

        for key in ['silhouette', 'davies_bouldin', 'calinski_harabasz']:
            val_without = metrics_without[key]
            val_with = metrics_with[key]

            if not np.isnan(val_without) and not np.isnan(val_with):
                if key == 'davies_bouldin':
                    improvement = (val_without - val_with) / val_without * 100
                    arrow = "â†“" if val_with < val_without else "â†‘"
                else:
                    improvement = (val_with - val_without) / abs(val_without) * 100
                    arrow = "â†‘" if val_with > val_without else "â†“"

                f.write(f"{key:<30} {val_without:<15.4f} {val_with:<15.4f} "
                       f"{arrow}{abs(improvement):<13.1f}%\n")

        f.write("\n" + "=" * 80 + "\n")
        f.write("ç±»åˆ†ç¦»åº¦æŒ‡æ ‡\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"{'æŒ‡æ ‡':<40} {'æ— èåˆ':<15} {'æœ‰èåˆ':<15} {'æ”¹è¿›':<15}\n")
        f.write("-" * 80 + "\n")

        # ç±»é—´è·ç¦»
        val_without = sep_without['inter_class_dist']
        val_with = sep_with['inter_class_dist']
        improvement = (val_with - val_without) / val_without * 100
        arrow = "â†‘" if val_with > val_without else "â†“"
        f.write(f"{'Inter-class Distance':<40} {val_without:<15.4f} "
               f"{val_with:<15.4f} {arrow}{abs(improvement):<13.1f}%\n")

        # å¹³å‡ç±»å†…è·ç¦»
        val_without = (sep_without['intra_class_dist_1'] + sep_without['intra_class_dist_2']) / 2
        val_with = (sep_with['intra_class_dist_1'] + sep_with['intra_class_dist_2']) / 2
        improvement = (val_without - val_with) / val_without * 100
        arrow = "â†“" if val_with < val_without else "â†‘"
        f.write(f"{'Avg Intra-class Distance':<40} {val_without:<15.4f} "
               f"{val_with:<15.4f} {arrow}{abs(improvement):<13.1f}%\n")

        # åˆ†ç¦»æ¯”ç‡
        val_without = sep_without['separation_ratio']
        val_with = sep_with['separation_ratio']
        improvement = (val_with - val_without) / val_without * 100
        arrow = "â†‘" if val_with > val_without else "â†“"
        f.write(f"{'Separation Ratio':<40} {val_without:<15.4f} "
               f"{val_with:<15.4f} {arrow}{abs(improvement):<13.1f}%\n")

    print(f"âœ“ ç»“æœæ‘˜è¦å·²ä¿å­˜: {summary_path}")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\nç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"  - hard_class_{class_pair[0]}_vs_{class_pair[1]}_balanced.png")
    print(f"  - separation_metrics_{class_pair[0]}_vs_{class_pair[1]}_balanced.png")
    print(f"  - summary_{class_pair[0]}_vs_{class_pair[1]}_balanced.txt")


if __name__ == '__main__':
    main()
