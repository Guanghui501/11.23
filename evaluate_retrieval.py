#!/usr/bin/env python
"""
Retrieval Evaluation Script for Graph-Text Alignment
è¯„ä¼°å›¾-æ–‡æœ¬å¯¹é½èƒ½åŠ›ï¼šR@1, R@5, R@10

è¿™ä¸ªè„šæœ¬å®ç°äº†"è¿è¿çœ‹"æ¸¸æˆï¼š
- ç»™å®š N ä¸ªå›¾å’Œ N æ®µæ–‡æœ¬
- å¯¹äºæ¯ä¸ªå›¾ï¼Œèƒ½å¦åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­æ‰¾åˆ°æ­£ç¡®åŒ¹é…ï¼Ÿ
- R@1: ç¬¬ä¸€åå°±æ˜¯æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
- R@5: å‰5ååŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
- R@10: å‰10ååŒ…å«æ­£ç¡®ç­”æ¡ˆçš„æ¯”ä¾‹
"""

import torch
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
import argparse
import json
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥ä½ çš„æ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
from models.alignn import ALIGNN, ALIGNNConfig
from data import get_train_val_loaders
from config import TrainingConfig


class RetrievalEvaluator:
    """å›¾-æ–‡æœ¬æ£€ç´¢è¯„ä¼°å™¨"""

    def __init__(self, model, device='cuda'):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨

        Args:
            model: è®­ç»ƒå¥½çš„ ALIGNN æ¨¡å‹
            device: 'cuda' æˆ– 'cpu'
        """
        self.model = model
        self.device = device
        self.model.eval()

    @torch.no_grad()
    def extract_features(self, dataloader, max_samples=None):
        """
        ä»æ•°æ®é›†æå–å›¾ç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾

        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            max_samples: æœ€å¤šæå–å¤šå°‘æ ·æœ¬ï¼ˆNone = å…¨éƒ¨ï¼‰

        Returns:
            graph_features: [N, feature_dim] å›¾ç‰¹å¾çŸ©é˜µ
            text_features: [N, feature_dim] æ–‡æœ¬ç‰¹å¾çŸ©é˜µ
            labels: [N] æ ‡ç­¾ï¼ˆç”¨äºåç»­åˆ†æï¼‰
        """
        print("ğŸ” æ­£åœ¨æå–ç‰¹å¾...")

        graph_features_list = []
        text_features_list = []
        labels_list = []

        total_samples = 0

        for batch_idx, (g, lg, text, labels) in enumerate(tqdm(dataloader)):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            g = g.to(self.device)
            lg = lg.to(self.device)

            # å‰å‘ä¼ æ’­ï¼Œè·å–ç‰¹å¾
            output_dict = self.model((g, lg, text), return_features=True)

            # æå–å›¾å’Œæ–‡æœ¬ç‰¹å¾
            graph_feat = output_dict['graph_features']  # [batch, 64]
            text_feat = output_dict['text_features']    # [batch, 64]

            # L2 å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            graph_feat = F.normalize(graph_feat, dim=1)
            text_feat = F.normalize(text_feat, dim=1)

            graph_features_list.append(graph_feat.cpu())
            text_features_list.append(text_feat.cpu())
            labels_list.append(labels.cpu())

            total_samples += graph_feat.size(0)

            # è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°åˆ™åœæ­¢
            if max_samples and total_samples >= max_samples:
                break

        # æ‹¼æ¥æ‰€æœ‰æ‰¹æ¬¡
        graph_features = torch.cat(graph_features_list, dim=0)
        text_features = torch.cat(text_features_list, dim=0)
        labels = torch.cat(labels_list, dim=0)

        # æˆªæ–­åˆ°æŒ‡å®šæ ·æœ¬æ•°
        if max_samples:
            graph_features = graph_features[:max_samples]
            text_features = text_features[:max_samples]
            labels = labels[:max_samples]

        print(f"âœ… æå–å®Œæˆ: {graph_features.size(0)} ä¸ªæ ·æœ¬")
        print(f"   - å›¾ç‰¹å¾ç»´åº¦: {graph_features.shape}")
        print(f"   - æ–‡æœ¬ç‰¹å¾ç»´åº¦: {text_features.shape}")

        return graph_features, text_features, labels

    def compute_similarity_matrix(self, graph_features, text_features):
        """
        è®¡ç®—å›¾-æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            graph_features: [N, D] å›¾ç‰¹å¾ï¼ˆå·²å½’ä¸€åŒ–ï¼‰
            text_features: [N, D] æ–‡æœ¬ç‰¹å¾ï¼ˆå·²å½’ä¸€åŒ–ï¼‰

        Returns:
            similarity_matrix: [N, N] ç›¸ä¼¼åº¦çŸ©é˜µ
                similarity[i, j] = cosine_similarity(graph_i, text_j)
        """
        print("ğŸ“Š è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")

        # ä½™å¼¦ç›¸ä¼¼åº¦ = å½’ä¸€åŒ–å‘é‡çš„ç‚¹ç§¯
        similarity_matrix = torch.matmul(graph_features, text_features.T)

        print(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µ: {similarity_matrix.shape}")
        return similarity_matrix

    def compute_retrieval_metrics(self, similarity_matrix, k_values=[1, 5, 10]):
        """
        è®¡ç®—æ£€ç´¢æŒ‡æ ‡ R@K

        Args:
            similarity_matrix: [N, N] ç›¸ä¼¼åº¦çŸ©é˜µ
            k_values: è¦è®¡ç®—çš„ K å€¼åˆ—è¡¨

        Returns:
            metrics: å­—å…¸ï¼ŒåŒ…å« Graph-to-Text å’Œ Text-to-Graph çš„ R@K
        """
        N = similarity_matrix.size(0)

        # æ­£ç¡®ç­”æ¡ˆçš„ç´¢å¼•ï¼ˆå¯¹è§’çº¿ï¼‰
        # ç¬¬ i ä¸ªå›¾å¯¹åº”ç¬¬ i ä¸ªæ–‡æœ¬
        correct_indices = torch.arange(N)

        metrics = {}

        # ========== Graph-to-Text æ£€ç´¢ ==========
        print("\nğŸ” Graph-to-Text æ£€ç´¢ï¼ˆç»™å®šå›¾ï¼Œæ‰¾æ–‡æœ¬ï¼‰:")

        # å¯¹æ¯ä¸€è¡Œæ’åºï¼ˆæ¯ä¸ªå›¾åœ¨æ‰€æœ‰æ–‡æœ¬ä¸­çš„ç›¸ä¼¼åº¦æ’åï¼‰
        # sorted_indices[i] = ç¬¬ i ä¸ªå›¾çš„æ–‡æœ¬æ’ååˆ—è¡¨
        _, sorted_indices = torch.sort(similarity_matrix, dim=1, descending=True)

        for k in k_values:
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨å‰ K å
            top_k_indices = sorted_indices[:, :k]  # [N, K]

            # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥æ­£ç¡®ç´¢å¼•æ˜¯å¦åœ¨ top-K ä¸­
            correct_in_top_k = (top_k_indices == correct_indices.unsqueeze(1)).any(dim=1)

            recall_at_k = correct_in_top_k.float().mean().item()
            metrics[f'g2t_R@{k}'] = recall_at_k

            print(f"   R@{k:2d} = {recall_at_k*100:.2f}%  "
                  f"({correct_in_top_k.sum().item()}/{N} æ ·æœ¬æˆåŠŸ)")

        # ========== Text-to-Graph æ£€ç´¢ ==========
        print("\nğŸ” Text-to-Graph æ£€ç´¢ï¼ˆç»™å®šæ–‡æœ¬ï¼Œæ‰¾å›¾ï¼‰:")

        # å¯¹æ¯ä¸€åˆ—æ’åºï¼ˆæ¯ä¸ªæ–‡æœ¬åœ¨æ‰€æœ‰å›¾ä¸­çš„ç›¸ä¼¼åº¦æ’åï¼‰
        _, sorted_indices = torch.sort(similarity_matrix, dim=0, descending=True)

        for k in k_values:
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨å‰ K å
            top_k_indices = sorted_indices[:k, :]  # [K, N]

            # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œæ£€æŸ¥æ­£ç¡®ç´¢å¼•æ˜¯å¦åœ¨ top-K ä¸­
            correct_in_top_k = (top_k_indices == correct_indices.unsqueeze(0)).any(dim=0)

            recall_at_k = correct_in_top_k.float().mean().item()
            metrics[f't2g_R@{k}'] = recall_at_k

            print(f"   R@{k:2d} = {recall_at_k*100:.2f}%  "
                  f"({correct_in_top_k.sum().item()}/{N} æ ·æœ¬æˆåŠŸ)")

        # ========== å¹³å‡æ£€ç´¢æ€§èƒ½ ==========
        print("\nğŸ“ˆ å¹³å‡æ£€ç´¢æ€§èƒ½:")
        for k in k_values:
            avg_recall = (metrics[f'g2t_R@{k}'] + metrics[f't2g_R@{k}']) / 2
            metrics[f'avg_R@{k}'] = avg_recall
            print(f"   Avg R@{k:2d} = {avg_recall*100:.2f}%")

        return metrics

    def analyze_failure_cases(self, similarity_matrix, graph_features, text_features,
                             labels, top_k=5):
        """
        åˆ†ææ£€ç´¢å¤±è´¥æ¡ˆä¾‹

        Args:
            similarity_matrix: [N, N] ç›¸ä¼¼åº¦çŸ©é˜µ
            graph_features, text_features: ç‰¹å¾çŸ©é˜µ
            labels: æ ‡ç­¾
            top_k: æ˜¾ç¤ºå‰ K ä¸ªæœ€å·®æ¡ˆä¾‹
        """
        N = similarity_matrix.size(0)
        correct_indices = torch.arange(N)

        print(f"\nâŒ åˆ†ææ£€ç´¢å¤±è´¥æ¡ˆä¾‹ï¼ˆæœ€å·® {top_k} ä¸ªï¼‰:")
        print("=" * 80)

        # Graph-to-Text æ£€ç´¢
        _, sorted_indices = torch.sort(similarity_matrix, dim=1, descending=True)

        # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
        ranks = []
        for i in range(N):
            correct_idx = correct_indices[i]
            rank = (sorted_indices[i] == correct_idx).nonzero(as_tuple=True)[0].item() + 1
            ranks.append(rank)

        ranks = torch.tensor(ranks)

        # æ‰¾å‡ºæœ€å·®çš„æ¡ˆä¾‹ï¼ˆæ’åæœ€é åï¼‰
        worst_indices = torch.argsort(ranks, descending=True)[:top_k]

        for idx in worst_indices:
            idx = idx.item()
            rank = ranks[idx].item()
            correct_sim = similarity_matrix[idx, idx].item()

            # æ‰¾å‡ºæ’åœ¨å‰é¢çš„é”™è¯¯åŒ¹é…
            top_wrong = sorted_indices[idx, 0].item()
            wrong_sim = similarity_matrix[idx, top_wrong].item()

            print(f"\næ ·æœ¬ {idx}:")
            print(f"  - çœŸå®æ ‡ç­¾: {labels[idx].item():.4f}")
            print(f"  - æ­£ç¡®åŒ¹é…æ’å: {rank} / {N}")
            print(f"  - æ­£ç¡®åŒ¹é…ç›¸ä¼¼åº¦: {correct_sim:.4f}")
            print(f"  - æœ€é«˜é”™è¯¯åŒ¹é… (ç´¢å¼• {top_wrong}): ç›¸ä¼¼åº¦ {wrong_sim:.4f}")
            print(f"  - ç›¸ä¼¼åº¦å·®è·: {wrong_sim - correct_sim:.4f}")

        # ç»Ÿè®¡æ’ååˆ†å¸ƒ
        print("\nğŸ“Š æ’ååˆ†å¸ƒ:")
        rank_bins = [1, 5, 10, 50, 100, N]
        for i in range(len(rank_bins) - 1):
            count = ((ranks > rank_bins[i]) & (ranks <= rank_bins[i+1])).sum().item()
            print(f"   æ’å {rank_bins[i]+1:4d}-{rank_bins[i+1]:4d}: {count:5d} æ ·æœ¬ "
                  f"({count/N*100:.1f}%)")

    def visualize_similarity_matrix(self, similarity_matrix, save_path=None,
                                   max_display=100):
        """
        å¯è§†åŒ–ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            similarity_matrix: [N, N] ç›¸ä¼¼åº¦çŸ©é˜µ
            save_path: ä¿å­˜è·¯å¾„
            max_display: æœ€å¤šæ˜¾ç¤ºå¤šå°‘ä¸ªæ ·æœ¬ï¼ˆé¿å…å›¾å¤ªå¤§ï¼‰
        """
        N = min(similarity_matrix.size(0), max_display)
        sim_matrix = similarity_matrix[:N, :N].numpy()

        plt.figure(figsize=(12, 10))

        # ç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(sim_matrix, cmap='RdYlGn', center=0,
                   vmin=-1, vmax=1, square=True,
                   cbar_kws={'label': 'Cosine Similarity'},
                   xticklabels=False, yticklabels=False)

        plt.title(f'Graph-Text Similarity Matrix (å‰ {N} ä¸ªæ ·æœ¬)\n'
                 f'å¯¹è§’çº¿ = æ­£ç¡®åŒ¹é…', fontsize=14, pad=20)
        plt.xlabel('Text Index', fontsize=12)
        plt.ylabel('Graph Index', fontsize=12)

        # æ·»åŠ å¯¹è§’çº¿æ ‡è®°
        plt.plot([0, N], [0, N], 'b--', linewidth=2, alpha=0.5, label='Perfect Alignment')
        plt.legend()

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ ç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜: {save_path}")

        plt.show()

    def visualize_retrieval_metrics(self, metrics, save_path=None):
        """
        å¯è§†åŒ–æ£€ç´¢æŒ‡æ ‡

        Args:
            metrics: æŒ‡æ ‡å­—å…¸
            save_path: ä¿å­˜è·¯å¾„
        """
        k_values = [1, 5, 10]

        g2t_values = [metrics[f'g2t_R@{k}'] * 100 for k in k_values]
        t2g_values = [metrics[f't2g_R@{k}'] * 100 for k in k_values]
        avg_values = [metrics[f'avg_R@{k}'] * 100 for k in k_values]

        x = np.arange(len(k_values))
        width = 0.25

        fig, ax = plt.subplots(figsize=(10, 6))

        bars1 = ax.bar(x - width, g2t_values, width, label='Graphâ†’Text',
                      color='steelblue', alpha=0.8)
        bars2 = ax.bar(x, t2g_values, width, label='Textâ†’Graph',
                      color='coral', alpha=0.8)
        bars3 = ax.bar(x + width, avg_values, width, label='Average',
                      color='mediumseagreen', alpha=0.8)

        ax.set_xlabel('K (Rank)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Recall@K (%)', fontsize=12, fontweight='bold')
        ax.set_title('å›¾-æ–‡æœ¬æ£€ç´¢æ€§èƒ½ (Retrieval Performance)',
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels([f'R@{k}' for k in k_values])
        ax.legend(fontsize=11)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        ax.set_ylim(0, 100)

        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        def autolabel(bars):
            for bar in bars:
                height = bar.get_height()
                ax.annotate(f'{height:.1f}%',
                          xy=(bar.get_x() + bar.get_width() / 2, height),
                          xytext=(0, 3),
                          textcoords="offset points",
                          ha='center', va='bottom',
                          fontsize=9)

        autolabel(bars1)
        autolabel(bars2)
        autolabel(bars3)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ’¾ æ£€ç´¢æŒ‡æ ‡å›¾å·²ä¿å­˜: {save_path}")

        plt.show()

    def evaluate(self, dataloader, max_samples=None, k_values=[1, 5, 10],
                visualize=True, output_dir='./retrieval_results'):
        """
        å®Œæ•´çš„æ£€ç´¢è¯„ä¼°æµç¨‹

        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
            max_samples: æœ€å¤šè¯„ä¼°å¤šå°‘æ ·æœ¬
            k_values: è¦è®¡ç®—çš„ K å€¼
            visualize: æ˜¯å¦å¯è§†åŒ–
            output_dir: ç»“æœè¾“å‡ºç›®å½•

        Returns:
            metrics: è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True, parents=True)

        print("=" * 80)
        print("ğŸ¯ å¼€å§‹å›¾-æ–‡æœ¬æ£€ç´¢è¯„ä¼°")
        print("=" * 80)

        # Step 1: æå–ç‰¹å¾
        graph_features, text_features, labels = self.extract_features(
            dataloader, max_samples
        )

        # Step 2: è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = self.compute_similarity_matrix(
            graph_features, text_features
        )

        # Step 3: è®¡ç®—æ£€ç´¢æŒ‡æ ‡
        metrics = self.compute_retrieval_metrics(similarity_matrix, k_values)

        # Step 4: åˆ†æå¤±è´¥æ¡ˆä¾‹
        self.analyze_failure_cases(
            similarity_matrix, graph_features, text_features, labels, top_k=5
        )

        # Step 5: å¯è§†åŒ–
        if visualize:
            print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–...")

            # ç›¸ä¼¼åº¦çŸ©é˜µ
            self.visualize_similarity_matrix(
                similarity_matrix,
                save_path=output_dir / 'similarity_matrix.png'
            )

            # æ£€ç´¢æŒ‡æ ‡
            self.visualize_retrieval_metrics(
                metrics,
                save_path=output_dir / 'retrieval_metrics.png'
            )

        # Step 6: ä¿å­˜ç»“æœ
        results = {
            'metrics': metrics,
            'num_samples': len(graph_features),
            'feature_dim': graph_features.size(1)
        }

        with open(output_dir / 'retrieval_results.json', 'w') as f:
            json.dump(results, f, indent=2)

        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        print("=" * 80)
        print("âœ… è¯„ä¼°å®Œæˆ!")
        print("=" * 80)

        return metrics


def main():
    """ä¸»å‡½æ•°ï¼šå‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(
        description='å›¾-æ–‡æœ¬æ£€ç´¢è¯„ä¼° (Retrieval Evaluation)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # è¯„ä¼°éªŒè¯é›†
  python evaluate_retrieval.py --checkpoint best_model.pt --split val

  # è¯„ä¼°æµ‹è¯•é›†ï¼Œæœ€å¤š 500 ä¸ªæ ·æœ¬
  python evaluate_retrieval.py --checkpoint best_model.pt --split test --max_samples 500

  # è®¡ç®— R@1, R@5, R@10, R@20
  python evaluate_retrieval.py --checkpoint best_model.pt --k_values 1 5 10 20
        """
    )

    parser.add_argument('--checkpoint', type=str, required=True,
                       help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„')
    parser.add_argument('--config', type=str, default='config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--split', type=str, default='val',
                       choices=['train', 'val', 'test'],
                       help='è¯„ä¼°å“ªä¸ªæ•°æ®é›†')
    parser.add_argument('--max_samples', type=int, default=None,
                       help='æœ€å¤šè¯„ä¼°å¤šå°‘æ ·æœ¬ï¼ˆNone = å…¨éƒ¨ï¼‰')
    parser.add_argument('--k_values', type=int, nargs='+', default=[1, 5, 10],
                       help='è®¡ç®— R@K çš„ K å€¼åˆ—è¡¨')
    parser.add_argument('--output_dir', type=str, default='./retrieval_results',
                       help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--no_visualize', action='store_true',
                       help='ä¸ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--device', type=str, default='cuda',
                       choices=['cuda', 'cpu'],
                       help='è¿è¡Œè®¾å¤‡')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®: {args.config}")
    # è¿™é‡Œå‡è®¾ä½ æœ‰ TrainingConfig ç±»
    # config = TrainingConfig.from_json(args.config)

    # åˆå§‹åŒ–æ¨¡å‹
    print(f"ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
    model_config = ALIGNNConfig(
        name="alignn",
        classification=True,
        use_cross_modal_attention=True,
        use_fine_grained_attention=False,
        use_middle_fusion=True,
        graph_dropout=0.0  # è¯„ä¼°æ—¶ä¸ç”¨ dropout
    )
    model = ALIGNN(model_config).to(device)

    # åŠ è½½æƒé‡
    print(f"ğŸ“¥ åŠ è½½æ£€æŸ¥ç‚¹: {args.checkpoint}")
    checkpoint = torch.load(args.checkpoint, map_location=device)
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½ {args.split} æ•°æ®é›†...")
    train_loader, val_loader, test_loader = get_train_val_loaders(
        dataset="your_dataset_path",  # æ ¹æ®ä½ çš„æ•°æ®é›†ä¿®æ”¹
        target="target_property",
        batch_size=32,
        workers=4
    )

    if args.split == 'train':
        dataloader = train_loader
    elif args.split == 'val':
        dataloader = val_loader
    else:
        dataloader = test_loader

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = RetrievalEvaluator(model, device)

    # è¿è¡Œè¯„ä¼°
    metrics = evaluator.evaluate(
        dataloader=dataloader,
        max_samples=args.max_samples,
        k_values=args.k_values,
        visualize=not args.no_visualize,
        output_dir=args.output_dir
    )

    # æ‰“å°æœ€ç»ˆç»“æœ
    print("\n" + "=" * 80)
    print("ğŸ“Š æœ€ç»ˆæ£€ç´¢æ€§èƒ½:")
    print("=" * 80)
    for key, value in sorted(metrics.items()):
        print(f"  {key:15s}: {value*100:6.2f}%")
    print("=" * 80)


if __name__ == '__main__':
    main()
