#!/usr/bin/env python
"""
ä¸­æœŸèåˆç‰¹å¾ç©ºé—´å¯è§†åŒ– - æŒ‰æ™¶ç³»èšç±»åˆ†æ
å¯¹æ¯”æœ‰/æ— ä¸­æœŸèåˆçš„ç‰¹å¾èšç±»è´¨é‡

ä½¿ç”¨æ–¹æ³•:
    python visualize_middle_fusion_clustering.py \
        --checkpoint_without_fusion model_no_fusion.pth \
        --checkpoint_with_fusion model_with_fusion.pth \
        --data_dir /path/to/dataset \
        --output_dir fusion_clustering_analysis
"""

import os
import argparse
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# é™ç»´ç®—æ³•
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("âš ï¸  UMAPæœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨t-SNE")

# å¯¼å…¥æ•°æ®å’Œæ¨¡å‹
from jarvis.core.atoms import Atoms
from data import get_train_val_loaders
from models.alignn import ALIGNN
from config import TrainingConfig

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['font.size'] = 11
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 13
plt.rcParams['figure.titlesize'] = 15
plt.rcParams['legend.fontsize'] = 10


# æ™¶ç³»å®šä¹‰
CRYSTAL_SYSTEMS = {
    'cubic': 'ç«‹æ–¹',
    'hexagonal': 'å…­æ–¹',
    'trigonal': 'ä¸‰æ–¹',
    'tetragonal': 'å››æ–¹',
    'orthorhombic': 'æ­£äº¤',
    'monoclinic': 'å•æ–œ',
    'triclinic': 'ä¸‰æ–œ'
}

CRYSTAL_SYSTEM_COLORS = {
    'cubic': '#e74c3c',        # çº¢è‰²
    'hexagonal': '#3498db',    # è“è‰²
    'trigonal': '#2ecc71',     # ç»¿è‰²
    'tetragonal': '#f39c12',   # æ©™è‰²
    'orthorhombic': '#9b59b6', # ç´«è‰²
    'monoclinic': '#1abc9c',   # é’è‰²
    'triclinic': '#e67e22'     # æ·±æ©™è‰²
}


def load_model(checkpoint_path, device='cpu'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"ğŸ“‚ åŠ è½½æ¨¡å‹: {checkpoint_path}")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)

    model_config = checkpoint.get('config', None)
    if model_config is None:
        raise ValueError("Checkpointä¸­æœªæ‰¾åˆ°æ¨¡å‹é…ç½®")

    model = ALIGNN(model_config)
    model.load_state_dict(checkpoint['model'])
    model.eval()
    model.to(device)

    # æ‰“å°ä¸­æœŸèåˆé…ç½®
    use_middle = model_config.use_middle_fusion if hasattr(model_config, 'use_middle_fusion') else False
    layers = model_config.middle_fusion_layers if hasattr(model_config, 'middle_fusion_layers') else 'N/A'
    print(f"   ä¸­æœŸèåˆ: {use_middle}")
    if use_middle:
        print(f"   èåˆå±‚: {layers}")

    return model, model_config


def extract_crystal_systems_from_dataset(dataset_array, cif_dir):
    """
    ä»dataset_arrayä¸­æå–æ™¶ç³»ä¿¡æ¯

    Returns:
        crystal_systems: æ™¶ç³»åˆ—è¡¨ï¼ˆä¸dataset_arrayé¡ºåºå¯¹åº”ï¼‰
        sample_ids: æ ·æœ¬IDåˆ—è¡¨
    """
    crystal_systems = []
    sample_ids = []

    print("ğŸ”„ ä»CIFæ–‡ä»¶æå–æ™¶ç³»ä¿¡æ¯...")

    for item in tqdm(dataset_array, desc="è¯»å–æ™¶ç³»"):
        sample_id = item['jid']
        sample_ids.append(sample_id)

        try:
            cif_file = os.path.join(cif_dir, f"{sample_id}.cif")
            if os.path.exists(cif_file):
                atoms = Atoms.from_cif(cif_file)
                crystal_system = atoms.lattice.lattice_system
                crystal_systems.append(crystal_system)
            else:
                crystal_systems.append('unknown')
        except Exception as e:
            crystal_systems.append('unknown')

    print(f"âœ… æ™¶ç³»æå–å®Œæˆ:")
    print(f"   æ€»æ ·æœ¬æ•°: {len(crystal_systems)}")
    print(f"   æ™¶ç³»åˆ†å¸ƒ:")
    for cs in sorted(set(crystal_systems)):
        count = crystal_systems.count(cs)
        print(f"     {CRYSTAL_SYSTEMS.get(cs, cs)}: {count}")

    return crystal_systems, sample_ids


def extract_features(model, data_loader, device='cpu'):
    """
    æå–ç‰¹å¾

    Returns:
        features: ç‰¹å¾çŸ©é˜µ [n_samples, n_features]
        targets: ç›®æ ‡å€¼
    """
    model.eval()
    features_list = []
    targets_list = []

    print("ğŸ”„ æå–ç‰¹å¾...")

    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(data_loader, desc="å¤„ç†æ‰¹æ¬¡")):
            try:
                # è§£åŒ…batch
                if len(batch) == 4:
                    g, lg, text, target = batch
                    model_input = (g.to(device), lg.to(device), text)
                else:
                    g, text, target = batch
                    model_input = (g.to(device), text)

                # å‰å‘ä¼ æ’­è·å–ç‰¹å¾
                output = model(model_input, return_features=True)

                # æå–èåˆç‰¹å¾
                if isinstance(output, dict):
                    if 'fused_features' in output:
                        feat = output['fused_features']
                    elif 'graph_features' in output:
                        feat = output['graph_features']
                    else:
                        # å°è¯•ä»è¾“å‡ºä¸­è·å–æœ€åçš„ç‰¹å¾
                        feat = output.get('features', None)
                        if feat is None:
                            print(f"âš ï¸  Batch {batch_idx}: æ— æ³•æå–ç‰¹å¾")
                            continue
                else:
                    # å¦‚æœè¿”å›çš„ä¸æ˜¯å­—å…¸ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
                    feat = output

                features_list.append(feat.cpu().numpy())
                targets_list.append(target.cpu().numpy())

            except Exception as e:
                print(f"âš ï¸  å¤„ç†batch {batch_idx}æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                continue

    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
    features = np.vstack(features_list)
    targets = np.concatenate(targets_list)

    print(f"âœ… æå–å®Œæˆ:")
    print(f"   ç‰¹å¾ç»´åº¦: {features.shape}")
    print(f"   æ ·æœ¬æ•°: {len(features)}")

    return features, targets


def compute_clustering_metrics(features, labels):
    """è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡"""
    # å°†å­—ç¬¦ä¸²æ ‡ç­¾è½¬ä¸ºæ•°å€¼
    unique_labels = list(set(labels))
    label_to_int = {label: i for i, label in enumerate(unique_labels)}
    numeric_labels = np.array([label_to_int[label] for label in labels])

    # è¿‡æ»¤æ‰unknownæ ‡ç­¾
    valid_mask = np.array(labels) != 'unknown'
    if valid_mask.sum() < 2:
        return {'silhouette': np.nan, 'davies_bouldin': np.nan, 'calinski_harabasz': np.nan}

    features_valid = features[valid_mask]
    labels_valid = numeric_labels[valid_mask]

    # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªç±»åˆ«
    if len(np.unique(labels_valid)) < 2:
        return {'silhouette': np.nan, 'davies_bouldin': np.nan, 'calinski_harabasz': np.nan}

    metrics = {}
    try:
        metrics['silhouette'] = silhouette_score(features_valid, labels_valid)
    except:
        metrics['silhouette'] = np.nan

    try:
        metrics['davies_bouldin'] = davies_bouldin_score(features_valid, labels_valid)
    except:
        metrics['davies_bouldin'] = np.nan

    try:
        metrics['calinski_harabasz'] = calinski_harabasz_score(features_valid, labels_valid)
    except:
        metrics['calinski_harabasz'] = np.nan

    return metrics


def apply_reduction(features, method='tsne', n_components=2):
    """é™ç»´"""
    print(f"ğŸ”„ åº”ç”¨{method.upper()}é™ç»´...")

    if method == 'tsne':
        reducer = TSNE(
            n_components=n_components,
            perplexity=min(30, len(features) - 1),
            random_state=42,
            max_iter=1000
        )
    elif method == 'umap' and UMAP_AVAILABLE:
        reducer = umap.UMAP(
            n_components=n_components,
            n_neighbors=min(15, len(features) - 1),
            min_dist=0.1,
            random_state=42
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é™ç»´æ–¹æ³•: {method}")

    embedded = reducer.fit_transform(features)
    print(f"âœ… é™ç»´å®Œæˆ: {embedded.shape}")

    return embedded


def plot_comparison(embedded_without, embedded_with, crystal_systems,
                   metrics_without, metrics_with, output_path):
    """
    åˆ›å»ºå¯¹æ¯”å›¾ï¼šæœ‰æ— ä¸­æœŸèåˆçš„ç‰¹å¾èšç±»å¯¹æ¯”
    """
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))

    # è¿‡æ»¤æ‰unknownçš„æ ·æœ¬ç”¨äºç»˜å›¾
    valid_mask = np.array(crystal_systems) != 'unknown'

    for idx, (embedded, metrics, title) in enumerate([
        (embedded_without, metrics_without, 'æ— ä¸­æœŸèåˆ'),
        (embedded_with, metrics_with, 'æœ‰ä¸­æœŸèåˆ')
    ]):
        ax = axes[idx]

        # ç»˜åˆ¶æ¯ä¸ªæ™¶ç³»
        for cs in set(crystal_systems):
            if cs == 'unknown':
                continue

            mask = (np.array(crystal_systems) == cs) & valid_mask
            if mask.sum() == 0:
                continue

            ax.scatter(
                embedded[mask, 0],
                embedded[mask, 1],
                c=CRYSTAL_SYSTEM_COLORS.get(cs, 'gray'),
                label=CRYSTAL_SYSTEMS.get(cs, cs),
                alpha=0.6,
                s=30,
                edgecolors='white',
                linewidths=0.5
            )

        ax.set_xlabel('ç»´åº¦ 1', fontsize=12)
        ax.set_ylabel('ç»´åº¦ 2', fontsize=12)
        ax.set_title(f'{title}\n' +
                    f'Silhouette: {metrics["silhouette"]:.3f} | ' +
                    f'DB: {metrics["davies_bouldin"]:.3f} | ' +
                    f'CH: {metrics["calinski_harabasz"]:.1f}',
                    fontsize=13, pad=15)
        ax.legend(loc='best', framealpha=0.9, fontsize=10)
        ax.grid(True, alpha=0.3)

    plt.suptitle('ç‰¹å¾ç©ºé—´èšç±»å¯¹æ¯” - æŒ‰æ™¶ç³»åˆ†ç»„', fontsize=16, y=0.98)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… å›¾åƒå·²ä¿å­˜: {output_path}")
    plt.close()


def plot_metrics_comparison(metrics_without, metrics_with, output_path):
    """ç»˜åˆ¶èšç±»æŒ‡æ ‡å¯¹æ¯”æŸ±çŠ¶å›¾"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    metric_names = ['Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Score']
    metric_keys = ['silhouette', 'davies_bouldin', 'calinski_harabasz']

    colors = ['#3498db', '#e74c3c']

    for idx, (name, key) in enumerate(zip(metric_names, metric_keys)):
        ax = axes[idx]
        values = [metrics_without[key], metrics_with[key]]
        bars = ax.bar(['æ— ä¸­æœŸèåˆ', 'æœ‰ä¸­æœŸèåˆ'], values, color=colors, alpha=0.7, edgecolor='black')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar in bars:
            height = bar.get_height()
            if not np.isnan(height):
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}',
                       ha='center', va='bottom', fontsize=11, fontweight='bold')

        ax.set_ylabel(name, fontsize=11)
        ax.set_title(name, fontsize=12, pad=10)
        ax.grid(True, axis='y', alpha=0.3)

        # Davies-Bouldin: è¶Šä½è¶Šå¥½
        if key == 'davies_bouldin':
            if values[1] < values[0]:
                ax.set_facecolor('#eafaf1')  # ç»¿è‰²èƒŒæ™¯è¡¨ç¤ºæ”¹è¿›

    plt.suptitle('èšç±»è´¨é‡æŒ‡æ ‡å¯¹æ¯”\n(Silhouetteå’ŒCHè¶Šé«˜è¶Šå¥½ï¼ŒDBè¶Šä½è¶Šå¥½)', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ… æŒ‡æ ‡å¯¹æ¯”å›¾å·²ä¿å­˜: {output_path}")
    plt.close()


def main():
    parser = argparse.ArgumentParser(description='ä¸­æœŸèåˆç‰¹å¾èšç±»å¯è§†åŒ–')
    parser.add_argument('--checkpoint_without_fusion', type=str, required=True,
                       help='æ— ä¸­æœŸèåˆçš„æ¨¡å‹checkpoint')
    parser.add_argument('--checkpoint_with_fusion', type=str, required=True,
                       help='æœ‰ä¸­æœŸèåˆçš„æ¨¡å‹checkpoint')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='æ•°æ®é›†ç›®å½•ï¼ˆåŒ…å«CIFæ–‡ä»¶ï¼‰')
    parser.add_argument('--dataset', type=str, default='jarvis',
                       help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--property', type=str, default='mbj_bandgap',
                       help='ç›®æ ‡å±æ€§')
    parser.add_argument('--n_samples', type=int, default=1000,
                       help='ç”¨äºå¯è§†åŒ–çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--reduction_method', type=str, default='tsne',
                       choices=['tsne', 'umap'], help='é™ç»´æ–¹æ³•')
    parser.add_argument('--output_dir', type=str, default='fusion_clustering_results',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®¾å¤‡')

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("=" * 80)
    print("ä¸­æœŸèåˆç‰¹å¾ç©ºé—´èšç±»åˆ†æ")
    print("=" * 80)

    # æ„å»ºCIFç›®å½•è·¯å¾„
    cif_dir = os.path.join(args.data_dir, f'{args.dataset}/{args.property}/cif/')

    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨æµ‹è¯•é›†ï¼‰
    print("\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®ä½ çš„æ•°æ®åŠ è½½é€»è¾‘è°ƒæ•´
    # ä¸ºäº†æ¼”ç¤ºï¼Œå‡è®¾ä½¿ç”¨test_loader

    # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
    print("\n" + "=" * 80)
    print("1ï¸âƒ£ åŠ è½½æ— ä¸­æœŸèåˆæ¨¡å‹")
    print("=" * 80)
    model_without, config_without = load_model(args.checkpoint_without_fusion, args.device)

    print("\n" + "=" * 80)
    print("2ï¸âƒ£ åŠ è½½æœ‰ä¸­æœŸèåˆæ¨¡å‹")
    print("=" * 80)
    model_with, config_with = load_model(args.checkpoint_with_fusion, args.device)

    # åŠ è½½æ•°æ®
    print("\n" + "=" * 80)
    print("3ï¸âƒ£ åŠ è½½æµ‹è¯•æ•°æ®")
    print("=" * 80)

    # ä½¿ç”¨train_mbj_with_optuna.pyä¸­çš„load_datasetå‡½æ•°
    try:
        import sys
        import csv
        from tqdm import tqdm

        # æ„å»ºæ•°æ®è·¯å¾„
        id_prop_file = os.path.join(args.data_dir, f'{args.dataset}/{args.property}/description.csv')

        print(f"CIFç›®å½•: {cif_dir}")
        print(f"æè¿°æ–‡ä»¶: {id_prop_file}")

        # ç®€åŒ–çš„æ•°æ®åŠ è½½ï¼ˆç›´æ¥åŠ è½½ï¼Œä¸éœ€è¦å¤æ‚çš„æ–‡æœ¬å¤„ç†ï¼‰
        print("åŠ è½½æ•°æ®é›†...")
        with open(id_prop_file, 'r') as f:
            reader = csv.reader(f)
            headings = next(reader)  # è·³è¿‡è¡¨å¤´
            data = [row for row in reader]

        print(f"æ€»æ ·æœ¬æ•°: {len(data)}")

        # é™åˆ¶æ ·æœ¬æ•°é‡
        if len(data) > args.n_samples:
            import random
            random.seed(42)
            data = random.sample(data, args.n_samples)
            print(f"éšæœºé€‰æ‹© {args.n_samples} ä¸ªæ ·æœ¬ç”¨äºå¯è§†åŒ–")

        # æ„å»ºdataset_array
        dataset_array = []
        skipped = 0

        for j in tqdm(range(len(data)), desc="åŠ è½½æ ·æœ¬"):
            try:
                if args.dataset.lower() == 'jarvis':
                    # JARVISæ ¼å¼: id, composition, target, description, file_name
                    sample_id = data[j][0]
                    composition = data[j][1]
                    target_val = float(data[j][2])
                    description = data[j][3]
                else:
                    # å…¶ä»–æ ¼å¼
                    sample_id = data[j][0]
                    target_val = float(data[j][1])
                    description = ""

                # è¯»å–CIFæ–‡ä»¶
                cif_file = os.path.join(cif_dir, f'{sample_id}.cif')
                if not os.path.exists(cif_file):
                    skipped += 1
                    continue

                atoms = Atoms.from_cif(cif_file)

                info = {
                    "atoms": atoms.to_dict(),
                    "jid": sample_id,
                    "text": description if description else atoms.composition.reduced_formula,
                    "target": target_val
                }

                dataset_array.append(info)

            except Exception as e:
                skipped += 1
                if skipped <= 5:
                    print(f"è·³è¿‡æ ·æœ¬ {j}: {e}")

        print(f"âœ“ æˆåŠŸåŠ è½½: {len(dataset_array)} æ ·æœ¬, è·³è¿‡: {skipped} æ ·æœ¬")

        if len(dataset_array) == 0:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ ·æœ¬ï¼")

        # è°ƒè¯•ï¼šæ£€æŸ¥æ•°æ®ç»“æ„
        print(f"\nè°ƒè¯•ä¿¡æ¯:")
        print(f"  æ•°æ®ç±»å‹: {type(dataset_array)}")
        print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬çš„é”®: {list(dataset_array[0].keys())}")
        print(f"  ç¬¬ä¸€ä¸ªæ ·æœ¬çš„targetå€¼: {dataset_array[0]['target']}")

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨ - ç›´æ¥åˆ›å»ºæµ‹è¯•é›†ï¼Œé¿å…ç©ºçš„è®­ç»ƒ/éªŒè¯é›†é—®é¢˜
        print("\nåˆ›å»ºæ•°æ®åŠ è½½å™¨...")

        # ä½¿ç”¨ get_torch_dataset ç›´æ¥åˆ›å»ºæ•°æ®é›†
        from data import get_torch_dataset
        from torch.utils.data import DataLoader

        test_data = get_torch_dataset(
            dataset=dataset_array,
            id_tag="jid",
            target="target",
            neighbor_strategy="k-nearest",
            atom_features="cgcnn",
            use_canonize=False,
            name=f"{args.dataset}_{args.property}",
            line_graph=True,
            cutoff=8.0,
            max_neighbors=12,
        )

        # åˆ›å»º DataLoader
        test_loader = DataLoader(
            test_data,
            batch_size=32,
            shuffle=False,
            num_workers=0,
            pin_memory=False,
            collate_fn=test_data.collate_line_graph,
        )

        # prepare_batch å‡½æ•°
        def prepare_batch(batch, device=args.device):
            """å‡†å¤‡æ‰¹æ¬¡æ•°æ®"""
            g, lg, text, target = batch
            g = g.to(device)
            lg = lg.to(device)
            target = target.to(device)
            return (g, lg, text), target

        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {len(test_data)} æ ·æœ¬")

        # æå–æ™¶ç³»ä¿¡æ¯ï¼ˆåœ¨åˆ›å»ºdata loaderä¹‹å‰ï¼Œä»åŸå§‹dataset_arrayï¼‰
        print("\n" + "=" * 80)
        print("4ï¸âƒ£ æå–æ™¶ç³»ä¿¡æ¯")
        print("=" * 80)
        crystal_systems, sample_ids = extract_crystal_systems_from_dataset(dataset_array, cif_dir)

    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        print("è¯·æ£€æŸ¥æ•°æ®è·¯å¾„å’Œæ ¼å¼")
        return

    # æå–ç‰¹å¾ - æ— ä¸­æœŸèåˆ
    print("\n" + "=" * 80)
    print("5ï¸âƒ£ æå–ç‰¹å¾ - æ— ä¸­æœŸèåˆæ¨¡å‹")
    print("=" * 80)
    features_without, targets = extract_features(
        model_without, test_loader, args.device
    )

    # æå–ç‰¹å¾ - æœ‰ä¸­æœŸèåˆ
    print("\n" + "=" * 80)
    print("6ï¸âƒ£ æå–ç‰¹å¾ - æœ‰ä¸­æœŸèåˆæ¨¡å‹")
    print("=" * 80)
    features_with, _ = extract_features(
        model_with, test_loader, args.device
    )

    # è®¡ç®—èšç±»æŒ‡æ ‡
    print("\n" + "=" * 80)
    print("7ï¸âƒ£ è®¡ç®—èšç±»æŒ‡æ ‡")
    print("=" * 80)

    print("æ— ä¸­æœŸèåˆ:")
    metrics_without = compute_clustering_metrics(features_without, crystal_systems)
    for metric, value in metrics_without.items():
        print(f"  {metric}: {value:.4f}")

    print("\næœ‰ä¸­æœŸèåˆ:")
    metrics_with = compute_clustering_metrics(features_with, crystal_systems)
    for metric, value in metrics_with.items():
        print(f"  {metric}: {value:.4f}")

    # é™ç»´
    print("\n" + "=" * 80)
    print("8ï¸âƒ£ é™ç»´å¯è§†åŒ–")
    print("=" * 80)

    embedded_without = apply_reduction(features_without, method=args.reduction_method, n_components=2)
    embedded_with = apply_reduction(features_with, method=args.reduction_method, n_components=2)

    # åˆ›å»ºå¯è§†åŒ–
    print("\n" + "=" * 80)
    print("9ï¸âƒ£ ç”Ÿæˆå¯è§†åŒ–å›¾åƒ")
    print("=" * 80)

    comparison_path = output_dir / "clustering_comparison.png"
    plot_comparison(embedded_without, embedded_with, crystal_systems,
                   metrics_without, metrics_with, comparison_path)

    metrics_path = output_dir / "metrics_comparison.png"
    plot_metrics_comparison(metrics_without, metrics_with, metrics_path)

    # ä¿å­˜ç»“æœæ‘˜è¦
    summary_path = output_dir / "summary.txt"
    with open(summary_path, 'w') as f:
        f.write("=" * 80 + "\n")
        f.write("ä¸­æœŸèåˆç‰¹å¾èšç±»åˆ†æç»“æœ\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"æ•°æ®é›†: {args.dataset} - {args.property}\n")
        f.write(f"æ ·æœ¬æ•°: {len(crystal_systems)}\n")
        f.write(f"é™ç»´æ–¹æ³•: {args.reduction_method.upper()}\n\n")

        f.write("=" * 80 + "\n")
        f.write("èšç±»æŒ‡æ ‡å¯¹æ¯”\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"{'æŒ‡æ ‡':<30} {'æ— èåˆ':<15} {'æœ‰èåˆ':<15} {'æ”¹è¿›':<15}\n")
        f.write("-" * 80 + "\n")

        for key in ['silhouette', 'davies_bouldin', 'calinski_harabasz']:
            val_without = metrics_without[key]
            val_with = metrics_with[key]

            if not np.isnan(val_without) and not np.isnan(val_with):
                if key == 'davies_bouldin':
                    improvement = (val_without - val_with) / val_without * 100
                    arrow = "â†“" if val_with < val_without else "â†‘"
                else:
                    improvement = (val_with - val_without) / abs(val_without) * 100
                    arrow = "â†‘" if val_with > val_without else "â†“"

                f.write(f"{key:<30} {val_without:<15.4f} {val_with:<15.4f} {arrow}{abs(improvement):<13.1f}%\n")
            else:
                f.write(f"{key:<30} {val_without:<15} {val_with:<15} {'N/A':<15}\n")

        f.write("\n" + "=" * 80 + "\n")
        f.write("æ™¶ç³»åˆ†å¸ƒ\n")
        f.write("=" * 80 + "\n\n")

        for cs in sorted(set(crystal_systems)):
            count = crystal_systems.count(cs)
            f.write(f"  {CRYSTAL_SYSTEMS.get(cs, cs):<15} {count:>6} æ ·æœ¬\n")

    print(f"âœ“ ç»“æœæ‘˜è¦å·²ä¿å­˜: {summary_path}")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("=" * 80)
    print(f"\nç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"  - clustering_comparison.png : èšç±»å¯¹æ¯”å›¾")
    print(f"  - metrics_comparison.png    : æŒ‡æ ‡å¯¹æ¯”å›¾")
    print(f"  - summary.txt               : ç»“æœæ‘˜è¦")


if __name__ == '__main__':
    main()
